<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "abf86d845c84330bce205a46b382ec88",
  "translation_date": "2025-10-11T11:42:07+00:00",
  "source_file": "2-Regression/4-Logistic/README.md",
  "language_code": "et"
}
-->
# Logistiline regressioon kategooriate ennustamiseks

![Logistilise ja lineaarse regressiooni infograafik](../../../../translated_images/linear-vs-logistic.ba180bf95e7ee66721ba10ebf2dac2666acbd64a88b003c83928712433a13c7d.et.png)

## [Eelloengu viktoriin](https://ff-quizzes.netlify.app/en/ml/)

> ### [See √µppetund on saadaval ka R-is!](../../../../2-Regression/4-Logistic/solution/R/lesson_4.html)

## Sissejuhatus

Selles viimases regressiooni √µppetunnis, mis on √ºks klassikalistest ML-tehnikatest, vaatame logistilist regressiooni. Seda tehnikat kasutatakse mustrite avastamiseks, et ennustada binaarseid kategooriaid. Kas see komm on ≈°okolaad v√µi mitte? Kas see haigus on nakkav v√µi mitte? Kas see klient valib selle toote v√µi mitte?

Selles √µppetunnis √µpid:

- Uut andmete visualiseerimise teeki
- Logistilise regressiooni tehnikaid

‚úÖ S√ºvenda oma arusaamist selle regressioonit√º√ºbi kasutamisest [Learn moodulis](https://docs.microsoft.com/learn/modules/train-evaluate-classification-models?WT.mc_id=academic-77952-leestott)

## Eeldused

Olles t√∂√∂tanud k√µrvitsate andmetega, oleme n√º√ºd piisavalt tuttavad, et m√µista, et seal on √ºks binaarne kategooria, millega saame t√∂√∂tada: `Color`.

Loome logistilise regressioonimudeli, et ennustada, millise v√§rviga k√µrvits t√µen√§oliselt on (oran≈æ üéÉ v√µi valge üëª), arvestades m√µningaid muutujaid.

> Miks me r√§√§gime binaarsest klassifikatsioonist regressiooni √µppetundide grupis? Lihtsalt keelelise mugavuse huvides, kuna logistiline regressioon on [tegelikult klassifikatsioonimeetod](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), kuigi lineaarne. √ïpi teisi viise andmete klassifitseerimiseks j√§rgmises √µppetundide grupis.

## K√ºsimuse m√§√§ratlemine

Meie eesm√§rkide jaoks v√§ljendame seda binaarselt: 'Valge' v√µi 'Mitte valge'. Meie andmestikus on ka 'triibuline' kategooria, kuid selle esinemisi on v√§he, seega me ei kasuta seda. See kaob niikuinii, kui eemaldame andmestikust nullv√§√§rtused.

> üéÉ L√µbus fakt: me kutsume valgeid k√µrvitsaid m√µnikord 'kummitus' k√µrvitsateks. Neid pole v√§ga lihtne nikerdada, seega pole nad nii populaarsed kui oran≈æid, kuid nad n√§evad lahedad v√§lja! Seega v√µiksime oma k√ºsimuse √ºmber s√µnastada: 'Kummitus' v√µi 'Mitte kummitus'. üëª

## Logistilise regressiooni kohta

Logistiline regressioon erineb lineaarse regressioonist, mida sa √µppisid varem, mitmel olulisel viisil.

[![ML algajatele - Logistilise regressiooni m√µistmine masin√µppe klassifikatsiooniks](https://img.youtube.com/vi/KpeCT6nEpBY/0.jpg)](https://youtu.be/KpeCT6nEpBY "ML algajatele - Logistilise regressiooni m√µistmine masin√µppe klassifikatsiooniks")

> üé• Kl√µpsa √ºlaloleval pildil, et vaadata l√ºhikest videot logistilise regressiooni √ºlevaatest.

### Binaarne klassifikatsioon

Logistiline regressioon ei paku samu funktsioone kui lineaarne regressioon. Esimene pakub ennustust binaarse kategooria kohta ("valge v√µi mitte valge"), samas kui teine suudab ennustada pidevaid v√§√§rtusi, n√§iteks arvestades k√µrvitsa p√§ritolu ja koristusaega, _kui palju selle hind t√µuseb_.

![K√µrvitsa klassifikatsioonimudel](../../../../translated_images/pumpkin-classifier.562771f104ad5436b87d1c67bca02a42a17841133556559325c0a0e348e5b774.et.png)
> Infograafik autorilt [Dasani Madipalli](https://twitter.com/dasani_decoded)

### Muud klassifikatsioonid

On olemas ka teisi logistilise regressiooni t√º√ºpe, sealhulgas multinomiaalne ja ordinaalne:

- **Multinomiaalne**, mis h√µlmab rohkem kui √ºhte kategooriat - "Oran≈æ, Valge ja Triibuline".
- **Ordinaalne**, mis h√µlmab j√§rjestatud kategooriaid, kasulik, kui tahame oma tulemusi loogiliselt j√§rjestada, n√§iteks k√µrvitsad, mis on j√§rjestatud piiratud arvu suuruste j√§rgi (mini, v√§ike, keskmine, suur, XL, XXL).

![Multinomiaalne vs ordinaalne regressioon](../../../../translated_images/multinomial-vs-ordinal.36701b4850e37d86c9dd49f7bef93a2f94dbdb8fe03443eb68f0542f97f28f29.et.png)

### Muutujad EI PEA korreleeruma

Kas m√§letad, kuidas lineaarne regressioon t√∂√∂tas paremini rohkem korreleeruvate muutujatega? Logistiline regressioon on vastupidine - muutujad ei pea olema seotud. See sobib selle andmestiku jaoks, millel on suhteliselt n√µrgad korrelatsioonid.

### Vajad palju puhast andmestikku

Logistiline regressioon annab t√§psemaid tulemusi, kui kasutad rohkem andmeid; meie v√§ike andmestik pole selle √ºlesande jaoks optimaalne, seega pea seda meeles.

[![ML algajatele - Andmete anal√º√ºs ja ettevalmistus logistilise regressiooni jaoks](https://img.youtube.com/vi/B2X4H9vcXTs/0.jpg)](https://youtu.be/B2X4H9vcXTs "ML algajatele - Andmete anal√º√ºs ja ettevalmistus logistilise regressiooni jaoks")

‚úÖ M√µtle, millised andmet√º√ºbid sobiksid h√§sti logistilisele regressioonile

## Harjutus - andmete korrastamine

K√µigepealt puhasta andmed veidi, eemaldades nullv√§√§rtused ja valides ainult m√µned veerud:

1. Lisa j√§rgmine kood:

    ```python
  
    columns_to_select = ['City Name','Package','Variety', 'Origin','Item Size', 'Color']
    pumpkins = full_pumpkins.loc[:, columns_to_select]

    pumpkins.dropna(inplace=True)
    ```

    Saad alati oma uuele andmestikule pilgu heita:

    ```python
    pumpkins.info
    ```

### Visualiseerimine - kategooriline graafik

Praeguseks oled taas laadinud [algusm√§rkmiku](./notebook.ipynb) k√µrvitsaandmetega ja puhastanud selle, et s√§ilitada andmestik, mis sisaldab m√µningaid muutujaid, sealhulgas `Color`. Visualiseerime andmestiku m√§rkmikus, kasutades teist teeki: [Seaborn](https://seaborn.pydata.org/index.html), mis on ehitatud Matplotlibi peale, mida kasutasime varem.

Seaborn pakub huvitavaid viise andmete visualiseerimiseks. N√§iteks saad v√µrrelda andmete jaotusi iga `Variety` ja `Color` jaoks kategoorilises graafikus.

1. Loo selline graafik, kasutades funktsiooni `catplot`, meie k√µrvitsaandmeid `pumpkins` ja m√§√§rates v√§rvikaardistuse iga k√µrvitsakategooria jaoks (oran≈æ v√µi valge):

    ```python
    import seaborn as sns
    
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }

    sns.catplot(
    data=pumpkins, y="Variety", hue="Color", kind="count",
    palette=palette, 
    )
    ```

    ![Visualiseeritud andmete ruudustik](../../../../translated_images/pumpkins_catplot_1.c55c409b71fea2ecc01921e64b91970542101f90bcccfa4aa3a205db8936f48b.et.png)

    Vaadates andmeid, n√§ed, kuidas `Color` andmed seostuvad `Variety`-ga.

    ‚úÖ Selle kategoorilise graafiku p√µhjal, milliseid huvitavaid uurimusi sa ette kujutad?

### Andmete eelt√∂√∂tlus: tunnuste ja siltide kodeerimine
Meie k√µrvitsaandmestik sisaldab stringiv√§√§rtusi k√µigi veergude jaoks. Kategooriliste andmetega t√∂√∂tamine on inimestele intuitiivne, kuid mitte masinatele. Masin√µppe algoritmid t√∂√∂tavad h√§sti numbritega. Seet√µttu on kodeerimine v√§ga oluline samm andmete eelt√∂√∂tluse faasis, kuna see v√µimaldab meil muuta kategoorilised andmed numbrilisteks andmeteks, kaotamata teavet. Hea kodeerimine aitab luua hea mudeli.

Tunnuste kodeerimiseks on kaks peamist t√º√ºpi kodeerijaid:

1. Ordinaalne kodeerija: see sobib h√§sti ordinaalsete muutujate jaoks, mis on kategoorilised muutujad, kus nende andmed j√§rgivad loogilist j√§rjestust, nagu `Item Size` veerg meie andmestikus. See loob kaardistuse, kus iga kategooriat esindab number, mis on kategooria j√§rjekord veerus.

    ```python
    from sklearn.preprocessing import OrdinalEncoder

    item_size_categories = [['sml', 'med', 'med-lge', 'lge', 'xlge', 'jbo', 'exjbo']]
    ordinal_features = ['Item Size']
    ordinal_encoder = OrdinalEncoder(categories=item_size_categories)
    ```

2. Kategooriline kodeerija: see sobib h√§sti nominaalsete muutujate jaoks, mis on kategoorilised muutujad, kus nende andmed ei j√§rgi loogilist j√§rjestust, nagu k√µik tunnused, mis erinevad `Item Size`-st meie andmestikus. See on √ºhekuum kodeerimine, mis t√§hendab, et iga kategooriat esindab binaarne veerg: kodeeritud muutuja on v√µrdne 1-ga, kui k√µrvits kuulub sellesse `Variety`-sse, ja 0-ga, kui mitte.

    ```python
    from sklearn.preprocessing import OneHotEncoder

    categorical_features = ['City Name', 'Package', 'Variety', 'Origin']
    categorical_encoder = OneHotEncoder(sparse_output=False)
    ```
Seej√§rel kasutatakse `ColumnTransformer`-it, et kombineerida mitu kodeerijat √ºheks sammuks ja rakendada neid sobivatele veergudele.

```python
    from sklearn.compose import ColumnTransformer
    
    ct = ColumnTransformer(transformers=[
        ('ord', ordinal_encoder, ordinal_features),
        ('cat', categorical_encoder, categorical_features)
        ])
    
    ct.set_output(transform='pandas')
    encoded_features = ct.fit_transform(pumpkins)
```
Teiselt poolt, sildi kodeerimiseks kasutame scikit-learn `LabelEncoder` klassi, mis on utiliitklass, mis aitab normaliseerida silte nii, et need sisaldaksid ainult v√§√§rtusi vahemikus 0 kuni n_classes-1 (siin, 0 ja 1).

```python
    from sklearn.preprocessing import LabelEncoder

    label_encoder = LabelEncoder()
    encoded_label = label_encoder.fit_transform(pumpkins['Color'])
```
Kui oleme tunnused ja sildi kodeerinud, saame need √ºhendada uueks andmestikuks `encoded_pumpkins`.

```python
    encoded_pumpkins = encoded_features.assign(Color=encoded_label)
```
‚úÖ Millised on ordinaalse kodeerija kasutamise eelised `Item Size` veeru jaoks?

### Muutujatevaheliste suhete anal√º√ºs

N√º√ºd, kui oleme oma andmed eelt√∂√∂tlenud, saame anal√º√ºsida tunnuste ja sildi vahelisi suhteid, et m√µista, kui h√§sti mudel suudab sildi ennustada, arvestades tunnuseid.
Parim viis sellise anal√º√ºsi tegemiseks on andmete graafikule kandmine. Kasutame taas Seaborn `catplot` funktsiooni, et visualiseerida suhteid `Item Size`, `Variety` ja `Color` vahel kategoorilises graafikus. Andmete paremaks graafikule kandmiseks kasutame kodeeritud `Item Size` veergu ja kodeerimata `Variety` veergu.

```python
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }
    pumpkins['Item Size'] = encoded_pumpkins['ord__Item Size']

    g = sns.catplot(
        data=pumpkins,
        x="Item Size", y="Color", row='Variety',
        kind="box", orient="h",
        sharex=False, margin_titles=True,
        height=1.8, aspect=4, palette=palette,
    )
    g.set(xlabel="Item Size", ylabel="").set(xlim=(0,6))
    g.set_titles(row_template="{row_name}")
```
![Visualiseeritud andmete kategooriline graafik](../../../../translated_images/pumpkins_catplot_2.87a354447880b3889278155957f8f60dd63db4598de5a6d0fda91c334d31f9f1.et.png)

### Kasuta 'swarm' graafikut

Kuna `Color` on binaarne kategooria (Valge v√µi Mitte), vajab see '[spetsiaalset l√§henemist](https://seaborn.pydata.org/tutorial/categorical.html?highlight=bar) visualiseerimiseks'. On ka teisi viise, kuidas visualiseerida selle kategooria suhet teiste muutujatega.

Saad visualiseerida muutujaid k√µrvuti Seaborn graafikutega.

1. Proovi 'swarm' graafikut, et n√§idata v√§√§rtuste jaotust:

    ```python
    palette = {
    0: 'orange',
    1: 'wheat'
    }
    sns.swarmplot(x="Color", y="ord__Item Size", data=encoded_pumpkins, palette=palette)
    ```

    ![Visualiseeritud andmete 'swarm'](../../../../translated_images/swarm_2.efeacfca536c2b577dc7b5f8891f28926663fbf62d893ab5e1278ae734ca104e.et.png)

**Ole ettevaatlik**: √ºlaltoodud kood v√µib genereerida hoiatuse, kuna Seaborn ei suuda esitada sellist hulka andmepunkte 'swarm' graafikus. V√µimalik lahendus on v√§hendada markeri suurust, kasutades 'size' parameetrit. Kuid ole teadlik, et see m√µjutab graafiku loetavust.

> **üßÆ N√§ita mulle matemaatikat**
>
> Logistiline regressioon tugineb 'maksimaalse t√µen√§osuse' kontseptsioonile, kasutades [sigmoidfunktsioone](https://wikipedia.org/wiki/Sigmoid_function). 'Sigmoidfunktsioon' graafikul n√§eb v√§lja nagu 'S'-kuju. See v√µtab v√§√§rtuse ja kaardistab selle vahemikku 0 kuni 1. Selle k√µverat nimetatakse ka 'logistiliseks k√µveraks'. Selle valem n√§eb v√§lja selline:
>
> ![logistiline funktsioon](../../../../translated_images/sigmoid.8b7ba9d095c789cf72780675d0d1d44980c3736617329abfc392dfc859799704.et.png)
>
> kus sigmoidi keskpunkt asub x-i 0 punktis, L on k√µvera maksimaalne v√§√§rtus ja k on k√µvera j√§rskus. Kui funktsiooni tulemus on suurem kui 0.5, antakse vastavale sildile binaarse valiku klass '1'. Kui mitte, klassifitseeritakse see kui '0'.

## Ehita oma mudel

Mudeli ehitamine nende binaarsete klassifikatsioonide leidmiseks on Scikit-learnis √ºllatavalt lihtne.

[![ML algajatele - Logistiline regressioon andmete klassifikatsiooniks](https://img.youtube.com/vi/MmZS2otPrQ8/0.jpg)](https://youtu.be/MmZS2otPrQ8 "ML algajatele - Logistiline regressioon andmete klassifikatsiooniks")

> üé• Kl√µpsa √ºlaloleval pildil, et vaadata l√ºhikest videot lineaarse regressioonimudeli ehitamisest

1. Vali muutujad, mida soovid kasutada oma klassifikatsioonimudelis, ja jaga treening- ja testkomplektid, kutsudes `train_test_split()`:

    ```python
    from sklearn.model_selection import train_test_split
    
    X = encoded_pumpkins[encoded_pumpkins.columns.difference(['Color'])]
    y = encoded_pumpkins['Color']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
    
    ```

2. N√º√ºd saad oma mudelit treenida, kutsudes `fit()` oma treeningandmetega, ja prindi selle tulemus:

    ```python
    from sklearn.metrics import f1_score, classification_report 
    from sklearn.linear_model import LogisticRegression

    model = LogisticRegression()
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)

    print(classification_report(y_test, predictions))
    print('Predicted labels: ', predictions)
    print('F1-score: ', f1_score(y_test, predictions))
    ```

    Vaata oma mudeli skooritabelit. See pole halb, arvestades, et sul on ainult umbes 1000 rida andmeid:

    ```output
                       precision    recall  f1-score   support
    
                    0       0.94      0.98      0.96       166
                    1       0.85      0.67      0.75        33
    
        accuracy                                0.92       199
        macro avg           0.89      0.82      0.85       199
        weighted avg        0.92      0.92      0.92       199
    
        Predicted labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0
        0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
        1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0
        0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0
        0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
        0 0 0 1 0 0 0 0 0 0 0 0 1 1]
        F1-score:  0.7457627118644068
    ```

## Parem arusaamine segadusmaatriksi kaudu

Kuigi saad skooritabeli aruande [terminid](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?highlight=classification_report#sklearn.metrics.classification_report) printides √ºlaltoodud √ºksused, v√µid oma mudelit paremini m√µista, kasutades [segadusmaatriksit](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix), et aidata meil m√µista, kuidas mudel toimib.

> üéì '[Segadusmaatriks](https://wikipedia.org/wiki/Confusion_matrix)' (v√µi 'veamaatriks') on tabel, mis v√§ljendab mudeli t√µelisi vs. valesid positiivseid ja negatiivseid tulemusi, hinnates seega ennustuste t√§psust.

1. Segadusmaatriksi kasutamiseks kutsu `confusion_matrix()`:

    ```python
    from sklearn.metrics import confusion_matrix
    confusion_matrix(y_test, predictions)
    ```

    Vaata oma mudeli segadusmaatriksit:

    ```output
    array([[162,   4],
           [ 11,  22]])
    ```

Scikit-learnis on segadusmaatriksis read (telg 0) tegelikud sildid ja veerud (telg 1) ennustatud sildid.

|       |   0   |   1   |
| :---: | :---: | :---: |
|   0   |  TN   |  FP   |
|   1   |  FN   |  TP   |

Mis siin toimub? Oletame, et meie mudelilt k√ºsitakse k√µrvitsate klassifitseerimist kahe binaarse kategooria vahel, kategooria 'valge' ja kategooria 'mitte-valge'.

- Kui mudel ennustab k√µrvitsa mitte valgeks ja see kuulub tegelikult kategooriasse 'mitte-valge', nimetame seda t√µeliseks negatiivseks, mida n√§itab √ºlemine vasakpoolne number.
- Kui mudel ennustab k√µrvitsa valgeks ja see kuulub tegelikult kategooriasse 'mitte-valge', nimetame seda valeks negatiivseks, mida n√§itab alumine vasakpoolne number.
- Kui mudel ennustab k√µrvitsa mitte valgeks ja see kuulub tegelikult kategooriasse 'valge', nimetame seda valeks positiivseks, mida n√§itab √ºlemine parempoolne number.
- Kui mudel ennustab k√µrvitsa valgeks ja see kuulub tegelikult kategooriasse 'valge', nimetame seda t√µeliseks positiivseks, mida n√§itab alumine parempoolne number.
Nagu v√µisite arvata, on eelistatav, et t√µeliste positiivsete ja t√µeliste negatiivsete arv oleks suurem ning valepositiivsete ja valenegatiivsete arv v√§iksem, mis viitab sellele, et mudel t√∂√∂tab paremini.

Kuidas on segadusmaatriks seotud t√§psuse ja tagasikutsumisega? Pidage meeles, et √ºlaltoodud klassifikatsiooniraport n√§itas t√§psust (0,85) ja tagasikutsumist (0,67).

T√§psus = tp / (tp + fp) = 22 / (22 + 4) = 0,8461538461538461

Tagasikutsumine = tp / (tp + fn) = 22 / (22 + 11) = 0,6666666666666666

‚úÖ K: Kuidas mudel segadusmaatriksi p√µhjal esines? V: Mitte halvasti; on palju t√µelisi negatiivseid, kuid ka m√µned valenegatiivsed.

Vaatame uuesti √ºle m√µisted, mida varem n√§gime, kasutades segadusmaatriksi TP/TN ja FP/FN kaardistust:

üéì T√§psus: TP/(TP + FP) Asjakohaste juhtumite osakaal leitud juhtumite hulgas (nt millised sildid olid √µigesti m√§rgistatud)

üéì Tagasikutsumine: TP/(TP + FN) Asjakohaste juhtumite osakaal, mis leiti, olenemata sellest, kas need olid √µigesti m√§rgistatud v√µi mitte

üéì f1-skoor: (2 * t√§psus * tagasikutsumine)/(t√§psus + tagasikutsumine) T√§psuse ja tagasikutsumise kaalutud keskmine, kus parim on 1 ja halvim 0

üéì Tugi: Iga leitud sildi esinemiste arv

üéì T√§psus: (TP + TN)/(TP + TN + FP + FN) Protsent siltidest, mis on proovi puhul √µigesti ennustatud.

üéì Makro keskmine: Iga sildi kaalumata keskmiste m√µ√µdikute arvutamine, arvestamata siltide tasakaalustamatust.

üéì Kaalutud keskmine: Iga sildi keskmiste m√µ√µdikute arvutamine, arvestades siltide tasakaalustamatust, kaaludes neid nende toe (iga sildi t√µeliste juhtumite arvu) j√§rgi.

‚úÖ Kas oskad arvata, millist m√µ√µdikut peaksid j√§lgima, kui soovid v√§hendada valenegatiivsete arvu?

## Visualiseeri selle mudeli ROC k√µverat

[![ML algajatele - logistilise regressiooni j√µudluse anal√º√ºs ROC k√µveratega](https://img.youtube.com/vi/GApO575jTA0/0.jpg)](https://youtu.be/GApO575jTA0 "ML algajatele - logistilise regressiooni j√µudluse anal√º√ºs ROC k√µveratega")

> üé• Kl√µpsa √ºlaloleval pildil, et vaadata l√ºhikest videot ROC k√µveratest

Teeme veel √ºhe visualiseerimise, et n√§ha nn 'ROC' k√µverat:

```python
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline

y_scores = model.predict_proba(X_test)
fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])

fig = plt.figure(figsize=(6, 6))
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()
```

Kasutades Matplotlibi, joonista mudeli [Vastuv√µtu T√∂√∂omaduste K√µver](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html?highlight=roc) ehk ROC. ROC k√µveraid kasutatakse sageli klassifikaatori v√§ljundi vaatamiseks t√µeliste ja valepositiivsete osas. "ROC k√µveratel on tavaliselt t√µeliste positiivsete m√§√§r Y-teljel ja valepositiivsete m√§√§r X-teljel." Seega on k√µvera j√§rskus ja kaugus keskjoonest k√µverani olulised: soovid k√µverat, mis kiiresti t√µuseb ja liigub √ºle joone. Meie puhul on alguses valepositiivsed, kuid seej√§rel t√µuseb joon korralikult √ºles ja √ºle:

![ROC](../../../../translated_images/ROC_2.777f20cdfc4988ca683ade6850ac832cb70c96c12f1b910d294f270ef36e1a1c.et.png)

L√µpuks kasuta Scikit-learn'i [`roc_auc_score` API-t](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html?highlight=roc_auc#sklearn.metrics.roc_auc_score), et arvutada tegelik 'K√µvera Alune Pindala' (AUC):

```python
auc = roc_auc_score(y_test,y_scores[:,1])
print(auc)
```
Tulemus on `0.9749908725812341`. Arvestades, et AUC ulatub 0-st 1-ni, soovid suurt skoori, kuna mudel, mis on oma ennustustes 100% t√§pne, saab AUC-iks 1; antud juhul on mudel _√ºsna hea_.

Tulevastes klassifikatsioonitundides √µpid, kuidas oma mudeli skoori parandamiseks iteratsioone teha. Aga praegu, palju √µnne! Oled need regressioonitunnid l√µpetanud!

---
## üöÄV√§ljakutse

Logistilises regressioonis on palju rohkem avastada! Kuid parim viis √µppimiseks on katsetamine. Leia andmestik, mis sobib selliseks anal√º√ºsiks, ja loo sellega mudel. Mida sa √µpid? Vihje: proovi [Kaggle](https://www.kaggle.com/search?q=logistic+regression+datasets), et leida huvitavaid andmestikke.

## [Loengu-j√§rgne viktoriin](https://ff-quizzes.netlify.app/en/ml/)

## √úlevaade ja iseseisev √µppimine

Loe [selle Stanfordi artikli](https://web.stanford.edu/~jurafsky/slp3/5.pdf) esimesi lehek√ºlgi, mis k√§sitlevad logistilise regressiooni praktilisi kasutusviise. M√µtle √ºlesannetele, mis sobivad paremini √ºhe v√µi teise t√º√ºpi regressiooni√ºlesannete jaoks, mida oleme seni √µppinud. Mis t√∂√∂taks k√µige paremini?

## √úlesanne

[Proovi seda regressiooni uuesti](assignment.md)

---

**Lahti√ºtlus**:  
See dokument on t√µlgitud AI t√µlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi p√º√ºame tagada t√§psust, palume arvestada, et automaatsed t√µlked v√µivad sisaldada vigu v√µi ebat√§psusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimt√µlget. Me ei vastuta selle t√µlke kasutamisest tulenevate arusaamatuste v√µi valesti t√µlgenduste eest.