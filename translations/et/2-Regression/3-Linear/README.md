<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "40e64f004f3cb50aa1d8661672d3cd92",
  "translation_date": "2025-10-11T11:46:03+00:00",
  "source_file": "2-Regression/3-Linear/README.md",
  "language_code": "et"
}
-->
# Ehita regressioonimudel Scikit-learniga: neli viisi regressiooniks

![Lineaarse ja pol√ºnoomse regressiooni infograafika](../../../../translated_images/linear-polynomial.5523c7cb6576ccab0fecbd0e3505986eb2d191d9378e785f82befcf3a578a6e7.et.png)
> Infograafika autor: [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Eelloengu viktoriin](https://ff-quizzes.netlify.app/en/ml/)

> ### [See √µppetund on saadaval ka R-is!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Sissejuhatus

Siiani olete uurinud, mis on regressioon, kasutades n√§idisandmeid k√µrvitsate hindade andmestikust, mida kasutame kogu selle √µppetunni jooksul. Olete seda visualiseerinud ka Matplotlibi abil.

N√º√ºd olete valmis s√ºvenema regressiooni masin√µppe jaoks. Kuigi visualiseerimine aitab andmetest aru saada, peitub masin√µppe t√µeline j√µud _mudelite treenimises_. Mudelid treenitakse ajalooliste andmete p√µhjal, et automaatselt tabada andmete s√µltuvusi, ja need v√µimaldavad ennustada tulemusi uute andmete p√µhjal, mida mudel pole varem n√§inud.

Selles √µppetunnis √µpite rohkem kahte t√º√ºpi regressiooni kohta: _lihtne lineaarne regressioon_ ja _pol√ºnoomne regressioon_, koos m√µningate matemaatiliste alustega, mis neid tehnikaid toetavad. Need mudelid v√µimaldavad meil ennustada k√µrvitsate hindu s√µltuvalt erinevatest sisendandmetest.

[![ML algajatele - Lineaarse regressiooni m√µistmine](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML algajatele - Lineaarse regressiooni m√µistmine")

> üé• Kl√µpsake √ºlaloleval pildil, et vaadata l√ºhikest videot lineaarse regressiooni √ºlevaatest.

> Kogu selle √µppekava jooksul eeldame minimaalset matemaatikaalast teadmist ja p√º√ºame muuta selle arusaadavaks teiste valdkondade √µpilastele, seega j√§lgige m√§rkmeid, üßÆ matemaatilisi vihjeid, diagramme ja muid √µppimisvahendeid, mis aitavad m√µistmist.

### Eeldused

Praeguseks peaksite olema tuttav k√µrvitsate andmete struktuuriga, mida me uurime. Need andmed on selle √µppetunni _notebook.ipynb_ failis eelnevalt laaditud ja puhastatud. Failis kuvatakse k√µrvitsate hind busheli kohta uues andmeraamis. Veenduge, et saate neid m√§rkmikke Visual Studio Code'i kerneli abil k√§ivitada.

### Ettevalmistus

Tuletame meelde, et laadite need andmed, et neilt k√ºsimusi k√ºsida.

- Millal on parim aeg k√µrvitsaid osta?
- Millist hinda v√µin oodata miniatuurse k√µrvitsate kasti eest?
- Kas peaksin ostma neid poole busheli korvides v√µi 1 1/9 busheli kastides?
Uurime neid andmeid edasi.

Eelmises √µppetunnis l√µite Pandase andmeraami ja t√§itsite selle osa algsest andmestikust, standardiseerides hinnad busheli j√§rgi. Sellega suutsime aga koguda ainult umbes 400 andmepunkti ja ainult s√ºgiskuude kohta.

Vaadake andmeid, mis on selle √µppetunni kaasasolevas m√§rkmikus eelnevalt laaditud. Andmed on eelnevalt laaditud ja algne hajuvusdiagramm on koostatud, et n√§idata kuude andmeid. V√µib-olla saame andmete olemuse kohta rohkem √ºksikasju, kui neid rohkem puhastame.

## Lineaarse regressiooni joon

Nagu √µppisite 1. √µppetunnis, on lineaarse regressiooni eesm√§rk joonistada joon, et:

- **N√§idata muutujate seoseid**. N√§idata muutujate vahelist seost
- **Teha ennustusi**. Teha t√§pseid ennustusi selle kohta, kuhu uus andmepunkt selle joone suhtes paigutuks.

T√º√ºpiline **v√§ikseimate ruutude regressioon** joonistab sellist joont. Termin "v√§ikseimad ruudud" t√§hendab, et k√µik regressioonijoone √ºmber olevad andmepunktid ruudustatakse ja seej√§rel liidetakse. Ideaalis on see l√µplik summa v√µimalikult v√§ike, kuna soovime v√§ikest vigade arvu ehk `v√§ikseimad ruudud`.

Teeme seda, kuna soovime modelleerida joont, millel on k√µigi meie andmepunktide suhtes k√µige v√§iksem kumulatiivne kaugus. Samuti ruudustame terminid enne nende liitmist, kuna meid huvitab nende suurus, mitte suund.

> **üßÆ N√§ita mulle matemaatikat**
>
> Seda joont, mida nimetatakse _parima sobivuse jooneks_, saab v√§ljendada [v√µrrandiga](https://en.wikipedia.org/wiki/Simple_linear_regression):
>
> ```
> Y = a + bX
> ```
>
> `X` on 'selgitav muutuja'. `Y` on 's√µltuv muutuja'. Joone kalle on `b` ja `a` on y-teljel√µige, mis viitab `Y` v√§√§rtusele, kui `X = 0`.
>
>![kalle arvutamine](../../../../translated_images/slope.f3c9d5910ddbfcf9096eb5564254ba22c9a32d7acd7694cab905d29ad8261db3.et.png)
>
> K√µigepealt arvutage kalle `b`. Infograafika autor: [Jen Looper](https://twitter.com/jenlooper)
>
> Teisis√µnu, viidates meie k√µrvitsate andmete algsele k√ºsimusele: "ennusta k√µrvitsa hinda busheli kohta kuu j√§rgi", viitaks `X` hinnale ja `Y` m√º√ºgikuule.
>
>![v√µrrandi t√§itmine](../../../../translated_images/calculation.a209813050a1ddb141cdc4bc56f3af31e67157ed499e16a2ecf9837542704c94.et.png)
>
> Arvutage Y v√§√§rtus. Kui maksate umbes 4 dollarit, peab olema aprill! Infograafika autor: [Jen Looper](https://twitter.com/jenlooper)
>
> Matemaatika, mis arvutab joone, peab n√§itama joone kallet, mis s√µltub ka l√µikepunktist ehk sellest, kus `Y` asub, kui `X = 0`.
>
> V√µite vaadata nende v√§√§rtuste arvutamise meetodit veebisaidil [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html). K√ºlastage ka [v√§ikseimate ruutude kalkulaatorit](https://www.mathsisfun.com/data/least-squares-calculator.html), et n√§ha, kuidas numbrite v√§√§rtused joont m√µjutavad.

## Korrelatsioon

Veel √ºks termin, mida m√µista, on **korrelatsioonikordaja** antud X ja Y muutujate vahel. Hajuvusdiagrammi abil saate kiiresti visualiseerida seda kordajat. Diagramm, mille andmepunktid on korrektselt joondatud, omab k√µrget korrelatsiooni, kuid diagramm, mille andmepunktid on X ja Y vahel laiali, omab madalat korrelatsiooni.

Hea lineaarse regressiooni mudel on selline, millel on k√µrge (l√§hemal 1-le kui 0-le) korrelatsioonikordaja, kasutades v√§ikseimate ruutude regressiooni meetodit koos regressioonijoonega.

‚úÖ K√§ivitage selle √µppetunni kaasasolev m√§rkmik ja vaadake kuude ja hindade hajuvusdiagrammi. Kas andmed, mis seostavad kuud ja hinda k√µrvitsate m√º√ºgi puhul, tunduvad teie visuaalse t√µlgenduse j√§rgi hajuvusdiagrammil k√µrge v√µi madala korrelatsiooniga? Kas see muutub, kui kasutate kuude asemel t√§psemat m√µ√µdet, n√§iteks *aasta p√§eva* (st p√§evade arv aasta algusest)?

Allolevas koodis eeldame, et oleme andmed puhastanud ja saanud andmeraami nimega `new_pumpkins`, mis n√§eb v√§lja umbes selline:

ID | Kuu | AastaP√§ev | Sort | Linn | Pakend | Madal Hind | K√µrge Hind | Hind
---|-----|-----------|------|------|--------|------------|------------|-----
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 busheli kastid | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 busheli kastid | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 busheli kastid | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 busheli kastid | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 busheli kastid | 15.0 | 15.0 | 13.636364

> Kood andmete puhastamiseks on saadaval failis [`notebook.ipynb`](notebook.ipynb). Oleme teinud samad puhastamistoimingud nagu eelmises √µppetunnis ja arvutanud `AastaP√§ev` veeru j√§rgmise avaldise abil:

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

N√º√ºd, kui olete aru saanud lineaarse regressiooni matemaatikast, loome regressioonimudeli, et n√§ha, kas suudame ennustada, milline k√µrvitsate pakend pakub parimaid k√µrvitsahindu. Keegi, kes ostab k√µrvitsaid p√ºhade k√µrvitsaplatsi jaoks, v√µib soovida seda teavet, et optimeerida k√µrvitsapakendite ostmist platsi jaoks.

## Korrelatsiooni otsimine

[![ML algajatele - Korrelatsiooni otsimine: lineaarse regressiooni v√µti](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML algajatele - Korrelatsiooni otsimine: lineaarse regressiooni v√µti")

> üé• Kl√µpsake √ºlaloleval pildil, et vaadata l√ºhikest videot korrelatsiooni √ºlevaatest.

Eelmises √µppetunnis olete t√µen√§oliselt n√§inud, et keskmine hind erinevate kuude kohta n√§eb v√§lja selline:

<img alt="Keskmine hind kuu j√§rgi" src="../../../../translated_images/barchart.a833ea9194346d769c77a3a870f7d8aee51574cd1138ca902e5500830a41cbce.et.png" width="50%"/>

See viitab sellele, et peaks olema mingi korrelatsioon, ja me v√µime proovida treenida lineaarse regressiooni mudelit, et ennustada seost `Kuu` ja `Hinna` vahel v√µi `AastaP√§eva` ja `Hinna` vahel. Siin on hajuvusdiagramm, mis n√§itab viimast seost:

<img alt="Hajuvusdiagramm hinna ja aasta p√§eva vahel" src="../../../../translated_images/scatter-dayofyear.bc171c189c9fd553fe93030180b9c00ed123148a577640e4d7481c4c01811972.et.png" width="50%" /> 

Vaatame, kas korrelatsioon on olemas, kasutades funktsiooni `corr`:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Tundub, et korrelatsioon on √ºsna v√§ike, -0.15 `Kuu` j√§rgi ja -0.17 `AastaP√§eva` j√§rgi, kuid v√µib olla veel √ºks oluline seos. Tundub, et erinevad k√µrvitsasordid moodustavad erinevaid hinnaklastrid. Selle h√ºpoteesi kinnitamiseks joonistame iga k√µrvitsakategooria erineva v√§rviga. Kui edastame `scatter` joonistamisfunktsioonile parameetri `ax`, saame k√µik punktid samale graafikule joonistada:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Hajuvusdiagramm hinna ja aasta p√§eva vahel" src="../../../../translated_images/scatter-dayofyear-color.65790faefbb9d54fb8f6223c566c445b9fac58a1c15f41f8641c3842af9d548b.et.png" width="50%" /> 

Meie uurimine viitab sellele, et sordil on m√º√ºgikuup√§evast suurem m√µju √ºldisele hinnale. Seda n√§eme ka tulpdiagrammist:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Tulpdiagramm hinna ja sordi vahel" src="../../../../translated_images/price-by-variety.744a2f9925d9bcb43a9a8c69469ce2520c9524fabfa270b1b2422cc2450d6d11.et.png" width="50%" /> 

Keskendume hetkeks ainult √ºhele k√µrvitsasordile, 'pie type', ja vaatame, millist m√µju kuup√§ev hinnale avaldab:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Hajuvusdiagramm hinna ja aasta p√§eva vahel" src="../../../../translated_images/pie-pumpkins-scatter.d14f9804a53f927e7fe39aa072486f4ed1bdd7f31c8bb08f476855f4b02350c3.et.png" width="50%" /> 

Kui arvutame n√º√ºd korrelatsiooni `Hinna` ja `AastaP√§eva` vahel, kasutades funktsiooni `corr`, saame tulemuseks umbes `-0.27` - mis t√§hendab, et ennustava mudeli treenimine on m√µistlik.

> Enne lineaarse regressiooni mudeli treenimist on oluline veenduda, et meie andmed on puhtad. Lineaarne regressioon ei t√∂√∂ta h√§sti puuduvate v√§√§rtustega, seega on m√µistlik k√µik t√ºhjad lahtrid eemaldada:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Teine l√§henemisviis oleks t√§ita need t√ºhjad v√§√§rtused vastava veeru keskmiste v√§√§rtustega.

## Lihtne lineaarne regressioon

[![ML algajatele - Lineaarne ja pol√ºnoomne regressioon Scikit-learniga](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML algajatele - Lineaarne ja pol√ºnoomne regressioon Scikit-learniga")

> üé• Kl√µpsake √ºlaloleval pildil, et vaadata l√ºhikest videot lineaarse ja pol√ºnoomse regressiooni √ºlevaatest.

Lineaarse regressiooni mudeli treenimiseks kasutame **Scikit-learn** teeki.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Alustame sisendv√§√§rtuste (omaduste) ja oodatud v√§ljundi (sildi) eraldamisest eraldi numpy massiividesse:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Pange t√§hele, et pidime sisendandmetele rakendama `reshape`, et lineaarse regressiooni pakett neid √µigesti m√µistaks. Lineaarne regressioon eeldab sisendina 2D-massiivi, kus massiivi iga rida vastab sisendi omaduste vektorile. Meie puhul, kuna meil on ainult √ºks sisend, vajame massiivi kujuga N&times;1, kus N on andmestiku suurus.

Seej√§rel peame andmed jagama treening- ja testandmestikeks, et saaksime p√§rast treenimist oma mudelit valideerida:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

L√µpuks v√µtab tegeliku lineaarse regressiooni mudeli treenimine ainult kaks koodirida. M√§√§ratleme `LinearRegression` objekti ja sobitame selle meie andmetega, kasutades meetodit `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

`LinearRegression` objekt p√§rast `fit`-imist sisaldab k√µiki regressiooni koefitsiente, millele p√§√§seb ligi omaduse `.coef_` kaudu. Meie puhul on ainult √ºks koefitsient, mis peaks olema umbes `-0.017`. See t√§hendab, et hinnad n√§ivad aja jooksul veidi langevat, kuid mitte liiga palju, umbes 2 senti p√§evas. Samuti p√§√§seme regressiooni l√µikepunktile Y-teljega, kasutades `lin_reg.intercept_` - see on meie puhul umbes `21`, mis n√§itab hinda aasta alguses.
Et n√§ha, kui t√§pne meie mudel on, saame prognoosida hindu testandmestikul ja seej√§rel m√µ√µta, kui l√§hedased meie prognoosid on oodatud v√§√§rtustele. Seda saab teha keskmise ruutvea (MSE) m√µ√µdiku abil, mis on k√µigi oodatud ja prognoositud v√§√§rtuste ruutude erinevuste keskmine.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

Meie viga tundub olevat umbes 2 punkti, mis on ~17%. Mitte just v√§ga hea. Teine mudeli kvaliteedi n√§itaja on **determinatsioonikordaja**, mida saab arvutada j√§rgmiselt:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```

Kui v√§√§rtus on 0, t√§hendab see, et mudel ei arvesta sisendandmeid ja toimib kui *halvim lineaarne ennustaja*, mis on lihtsalt tulemuse keskmine v√§√§rtus. V√§√§rtus 1 t√§hendab, et suudame t√§iuslikult prognoosida k√µiki oodatud v√§ljundeid. Meie puhul on determinatsioonikordaja umbes 0.06, mis on √ºsna madal.

Samuti saame testandmed koos regressioonijoonega graafikule panna, et paremini n√§ha, kuidas regressioon meie puhul toimib:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Lineaarne regressioon" src="../../../../translated_images/linear-results.f7c3552c85b0ed1ce2808276c870656733f6878c8fd37ec220812ee77686c3ef.et.png" width="50%" />

## Pol√ºnoomiline regressioon

Teine lineaarse regressiooni t√º√ºp on pol√ºnoomiline regressioon. Kuigi vahel on muutujate vahel lineaarne seos ‚Äì n√§iteks mida suurem k√µrvitsa maht, seda k√µrgem hind ‚Äì ei saa neid seoseid alati kujutada tasapinnana v√µi sirgjoonena.

‚úÖ Siin on [veel m√µned n√§ited](https://online.stat.psu.edu/stat501/lesson/9/9.8) andmetest, mis v√µiksid kasutada pol√ºnoomilist regressiooni.

Vaata uuesti seost kuup√§eva ja hinna vahel. Kas see hajusdiagramm tundub, et seda peaks tingimata anal√º√ºsima sirgjoonega? Kas hinnad ei v√µiks k√µikuda? Sellisel juhul v√µid proovida pol√ºnoomilist regressiooni.

‚úÖ Pol√ºnoomid on matemaatilised avaldised, mis v√µivad koosneda √ºhest v√µi mitmest muutujast ja kordajast.

Pol√ºnoomiline regressioon loob k√µvera joone, et paremini sobitada mittelineaarseid andmeid. Meie puhul, kui lisame sisendandmetesse ruutv√µrrandi `DayOfYear`, peaksime suutma oma andmeid sobitada paraboolse k√µveraga, millel on aasta teatud punktis miinimum.

Scikit-learn sisaldab kasulikku [pipeline API-d](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline), et kombineerida erinevaid andmet√∂√∂tluse samme. **Pipeline** on **hinnangute** ahel. Meie puhul loome pipeline'i, mis k√µigepealt lisab mudelile pol√ºnoomilised omadused ja seej√§rel treenib regressiooni:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

`PolynomialFeatures(2)` kasutamine t√§hendab, et lisame sisendandmetest k√µik teise astme pol√ºnoomid. Meie puhul t√§hendab see lihtsalt `DayOfYear`<sup>2</sup>, kuid kahe sisendmuutuja X ja Y korral lisab see X<sup>2</sup>, XY ja Y<sup>2</sup>. V√µime kasutada ka k√µrgema astme pol√ºnoome, kui soovime.

Pipeline'i saab kasutada samamoodi nagu algset `LinearRegression` objekti, st saame pipeline'i `fit`-ida ja seej√§rel kasutada `predict`, et saada prognoositulemused. Siin on graafik, mis n√§itab testandmeid ja l√§hendusk√µverat:

<img alt="Pol√ºnoomiline regressioon" src="../../../../translated_images/poly-results.ee587348f0f1f60bd16c471321b0b2f2457d0eaa99d99ec0ced4affc900fa96c.et.png" width="50%" />

Pol√ºnoomilist regressiooni kasutades saame veidi madalama MSE ja k√µrgema determinatsiooni, kuid mitte m√§rkimisv√§√§rselt. Peame arvesse v√µtma ka teisi omadusi!

> N√§ed, et minimaalsed k√µrvitsahinnad on t√§heldatud kuskil Halloweeni paiku. Kuidas sa seda selgitaksid?

üéÉ Palju √µnne, sa l√µid mudeli, mis aitab prognoosida pirukak√µrvitsate hinda. T√µen√§oliselt saad sama protseduuri korrata k√µigi k√µrvitsat√º√ºpide puhul, kuid see oleks t√ºlikas. √ïpime n√º√ºd, kuidas arvestada k√µrvitsasorti oma mudelis!

## Kategoorilised omadused

Ideaalis tahame olla v√µimelised prognoosima hindu erinevate k√µrvitsasortide jaoks, kasutades sama mudelit. Kuid `Variety` veerg erineb veidi veergudest nagu `Month`, kuna see sisaldab mitte-numerilisi v√§√§rtusi. Selliseid veerge nimetatakse **kategoorilisteks**.

[![ML algajatele - Kategooriliste omaduste prognoosimine lineaarse regressiooniga](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML algajatele - Kategooriliste omaduste prognoosimine lineaarse regressiooniga")

> üé• Kl√µpsa √ºlaloleval pildil, et vaadata l√ºhikest videot kategooriliste omaduste kasutamisest.

Siin n√§ed, kuidas keskmine hind s√µltub sordist:

<img alt="Keskmine hind sordi j√§rgi" src="../../../../translated_images/price-by-variety.744a2f9925d9bcb43a9a8c69469ce2520c9524fabfa270b1b2422cc2450d6d11.et.png" width="50%" />

Sordi arvesse v√µtmiseks peame esmalt selle numbriliseks vormiks teisendama ehk **kodeerima**. Selleks on mitu v√µimalust:

* Lihtne **numbriline kodeerimine** loob tabeli erinevatest sortidest ja asendab sordinime selle tabeli indeksiga. See pole lineaarse regressiooni jaoks parim idee, kuna lineaarne regressioon v√µtab indeksi tegeliku numbrilise v√§√§rtuse ja lisab selle tulemusele, korrutades mingi kordajaga. Meie puhul on indeksi numbri ja hinna vaheline seos selgelt mittelineaarne, isegi kui tagame, et indeksid on j√§rjestatud mingil konkreetsel viisil.
* **√úks-√ºhele kodeerimine** asendab `Variety` veeru nelja erineva veeruga, √ºks iga sordi jaoks. Iga veerg sisaldab `1`, kui vastav rida kuulub antud sordile, ja `0` muidu. See t√§hendab, et lineaarse regressiooni korral on neli kordajat, √ºks iga k√µrvitsasordi jaoks, mis vastutavad selle konkreetse sordi "algushinna" (v√µi pigem "lisahinna") eest.

Allolev kood n√§itab, kuidas saame sordi √ºks-√ºhele kodeerida:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

Lineaarse regressiooni treenimiseks, kasutades √ºks-√ºhele kodeeritud sorti sisendina, peame lihtsalt `X` ja `y` andmed √µigesti initsialiseerima:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

√úlej√§√§nud kood on sama, mida kasutasime √ºlal lineaarse regressiooni treenimiseks. Kui proovid seda, n√§ed, et keskmine ruutviga on umbes sama, kuid saame palju k√µrgema determinatsioonikordaja (~77%). T√§psemate prognooside saamiseks saame arvesse v√µtta rohkem kategoorilisi omadusi, samuti numbrilisi omadusi, nagu `Month` v√µi `DayOfYear`. √úhe suure omaduste massiivi saamiseks saame kasutada `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Siin v√µtame arvesse ka `City` ja `Package` t√º√ºpi, mis annab meile MSE 2.84 (10%) ja determinatsiooni 0.94!

## K√µik kokku

Parima mudeli loomiseks saame kasutada kombineeritud (√ºks-√ºhele kodeeritud kategoorilised + numbrilised) andmeid √ºlaltoodud n√§itest koos pol√ºnoomilise regressiooniga. Siin on t√§ielik kood sinu mugavuseks:

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

See peaks andma meile parima determinatsioonikordaja, peaaegu 97%, ja MSE=2.23 (~8% prognoosiviga).

| Mudel | MSE | Determinatsioon |
|-------|-----|-----------------|
| `DayOfYear` Lineaarne | 2.77 (17.2%) | 0.07 |
| `DayOfYear` Pol√ºnoomiline | 2.73 (17.0%) | 0.08 |
| `Variety` Lineaarne | 5.24 (19.7%) | 0.77 |
| K√µik omadused Lineaarne | 2.84 (10.5%) | 0.94 |
| K√µik omadused Pol√ºnoomiline | 2.23 (8.25%) | 0.97 |

üèÜ Tubli t√∂√∂! L√µid √ºhe tunni jooksul neli regressioonimudelit ja parandasid mudeli kvaliteeti 97%-ni. Regressiooni viimases osas √µpid logistilist regressiooni kategooriate m√§√§ramiseks.

---
## üöÄV√§ljakutse

Testi mitmeid erinevaid muutujaid selles m√§rkmikus, et n√§ha, kuidas korrelatsioon vastab mudeli t√§psusele.

## [Loengu j√§rgne viktoriin](https://ff-quizzes.netlify.app/en/ml/)

## √úlevaade ja iseseisev √µppimine

Selles tunnis √µppisime lineaarset regressiooni. On ka teisi olulisi regressiooni t√º√ºpe. Loe Stepwise, Ridge, Lasso ja Elasticnet tehnikate kohta. Hea kursus, mida √µppida, on [Stanfordi statistilise √µppimise kursus](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).

## √úlesanne

[Ehita mudel](assignment.md)

---

**Lahti√ºtlus**:  
See dokument on t√µlgitud AI t√µlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi p√º√ºame tagada t√§psust, palume arvestada, et automaatsed t√µlked v√µivad sisaldada vigu v√µi ebat√§psusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimt√µlget. Me ei vastuta selle t√µlke kasutamisest tulenevate arusaamatuste v√µi valesti t√µlgenduste eest.