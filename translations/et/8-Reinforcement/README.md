<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "20ca019012b1725de956681d036d8b18",
  "translation_date": "2025-10-11T11:14:48+00:00",
  "source_file": "8-Reinforcement/README.md",
  "language_code": "et"
}
-->
# Sissejuhatus tugevdus√µppesse

Tugevdus√µpe, RL, on √ºks p√µhilisi masin√µppe paradigmasid, k√µrvuti juhendatud ja juhendamata √µppega. RL keskendub otsustele: √µigete otsuste tegemisele v√µi v√§hemalt nende √µppimisele.

Kujutlege, et teil on simuleeritud keskkond, n√§iteks aktsiaturg. Mis juhtub, kui kehtestate teatud regulatsiooni? Kas sellel on positiivne v√µi negatiivne m√µju? Kui juhtub midagi negatiivset, peate v√µtma selle _negatiivse tugevduse_, sellest √µppima ja suunda muutma. Kui tulemus on positiivne, peate sellele _positiivsele tugevdusele_ tuginedes edasi liikuma.

![Peeter ja hunt](../../../translated_images/peter.779730f9ba3a8a8d9290600dcf55f2e491c0640c785af7ac0d64f583c49b8864.et.png)

> Peeter ja tema s√µbrad peavad p√µgenema n√§ljase hundi eest! Pildi autor [Jen Looper](https://twitter.com/jenlooper)

## Regionaalne teema: Peeter ja hunt (Venemaa)

[Peeter ja hunt](https://en.wikipedia.org/wiki/Peter_and_the_Wolf) on muinasjutt, mille kirjutas vene helilooja [Sergei Prokofjev](https://en.wikipedia.org/wiki/Sergei_Prokofiev). See on lugu noorest pioneerist Peetrist, kes julgesti lahkub oma kodust, et metsas hundiga silmitsi seista. Selles osas treenime masin√µppe algoritme, mis aitavad Peetril:

- **Avastada** √ºmbritsevat ala ja koostada optimaalne navigeerimiskaart
- **√ïppida** kasutama rulaga tasakaalu hoidmist, et kiiremini liikuda.

[![Peeter ja hunt](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

> üé• Kl√µpsake √ºlaloleval pildil, et kuulata Prokofjevi "Peeter ja hunt"

## Tugevdus√µpe

Eelnevates osades olete n√§inud kahte masin√µppe probleemi n√§idet:

- **Juhendatud √µpe**, kus meil on andmekogumid, mis pakuvad n√§idislahendusi probleemile, mida soovime lahendada. [Klassifikatsioon](../4-Classification/README.md) ja [regressioon](../2-Regression/README.md) on juhendatud √µppe √ºlesanded.
- **Juhendamata √µpe**, kus meil ei ole m√§rgistatud treeningandmeid. Juhendamata √µppe peamine n√§ide on [klasterdamine](../5-Clustering/README.md).

Selles osas tutvustame teile uut t√º√ºpi √µppeprobleemi, mis ei vaja m√§rgistatud treeningandmeid. Selliseid probleeme on mitut t√º√ºpi:

- **[Pooljuhendatud √µpe](https://wikipedia.org/wiki/Semi-supervised_learning)**, kus meil on palju m√§rgistamata andmeid, mida saab kasutada mudeli eeltreenimiseks.
- **[Tugevdus√µpe](https://wikipedia.org/wiki/Reinforcement_learning)**, kus agent √µpib k√§itumist, tehes katseid mingis simuleeritud keskkonnas.

### N√§ide - arvutim√§ng

Oletame, et soovite √µpetada arvutit m√§ngima m√§ngu, n√§iteks malet v√µi [Super Mario](https://wikipedia.org/wiki/Super_Mario). Selleks, et arvuti m√§ngu m√§ngiks, peame √µpetama seda ennustama, millist k√§iku teha igas m√§ngu seisus. Kuigi see v√µib tunduda klassifikatsiooniprobleemina, ei ole see nii - sest meil ei ole andmekogumit, mis sisaldaks seisusid ja vastavaid tegevusi. Kuigi meil v√µib olla m√µningaid andmeid, nagu olemasolevad malem√§ngud v√µi Super Mario m√§ngijate salvestused, ei kata need andmed t√µen√§oliselt piisavalt suurt hulka v√µimalikke seisusid.

Selle asemel, et otsida olemasolevaid m√§nguandmeid, p√µhineb **Tugevdus√µpe** (RL) ideel *lasta arvutil m√§ngida* mitu korda ja j√§lgida tulemust. Seega, et rakendada tugevdus√µpet, vajame kahte asja:

- **Keskkonda** ja **simulaatorit**, mis v√µimaldavad meil m√§ngu mitu korda m√§ngida. See simulaator m√§√§ratleks k√µik m√§ngureeglid, samuti v√µimalikud seisud ja tegevused.

- **Tasu funktsiooni**, mis √ºtleks meile, kui h√§sti meil iga k√§igu v√µi m√§ngu ajal l√§ks.

Peamine erinevus teiste masin√µppe t√º√ºpide ja RL vahel on see, et RL-is me tavaliselt ei tea, kas v√µidame v√µi kaotame, kuni m√§ng on l√µppenud. Seega ei saa me √∂elda, kas teatud k√§ik iseenesest on hea v√µi mitte - me saame tasu alles m√§ngu l√µpus. Meie eesm√§rk on kujundada algoritme, mis v√µimaldavad meil treenida mudelit ebakindlates tingimustes. √ïpime tundma √ºhte RL algoritmi, mida nimetatakse **Q-√µppeks**.

## √ïppetunnid

1. [Sissejuhatus tugevdus√µppesse ja Q-√µppesse](1-QLearning/README.md)
2. [Simulatsioonikeskkonna kasutamine Gymis](2-Gym/README.md)

## Autorid

"Sissejuhatus tugevdus√µppesse" on kirjutatud ‚ô•Ô∏è poolt [Dmitry Soshnikov](http://soshnikov.com)

---

**Lahti√ºtlus**:  
See dokument on t√µlgitud AI t√µlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi p√º√ºame tagada t√§psust, palume arvestada, et automaatsed t√µlked v√µivad sisaldada vigu v√µi ebat√§psusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimt√µlget. Me ei vastuta selle t√µlke kasutamisest tulenevate arusaamatuste v√µi valesti t√µlgenduste eest.