<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "911efd5e595089000cb3c16fce1beab8",
  "translation_date": "2025-10-11T11:19:54+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "et"
}
-->
# Sissejuhatus tugevdus√µppesse ja Q-√µppesse

![Tugevdus√µppe kokkuv√µte masin√µppes sketchnote'is](../../../../translated_images/ml-reinforcement.94024374d63348dbb3571c343ca7ddabef72adac0b8086d47164b769ba3a8a1d.et.png)
> Sketchnote autor: [Tomomi Imura](https://www.twitter.com/girlie_mac)

Tugevdus√µpe h√µlmab kolme olulist m√µistet: agent, teatud seisundid ja tegevuste kogum iga seisundi kohta. Kui agent sooritab kindlas seisundis tegevuse, saab ta tasu. Kujutle n√§iteks arvutim√§ngu Super Mario. Sina oled Mario, oled m√§ngutasemel ja seisad kaljuserval. Sinu kohal on m√ºnt. Sina, olles Mario, m√§ngutasemel kindlas asukohas ... see on sinu seisund. Kui liigud √ºhe sammu paremale (tegevus), kukud kaljult alla ja saad madala punktisumma. Kui aga vajutad h√ºppenuppu, saad punkti ja j√§√§d ellu. See on positiivne tulemus ja selle eest peaksid saama positiivse punktisumma.

Kasutades tugevdus√µpet ja simulaatorit (m√§ngu), saad √µppida, kuidas m√§ngu m√§ngida, et maksimeerida tasu, mis t√§hendab ellu j√§√§mist ja v√µimalikult paljude punktide kogumist.

[![Sissejuhatus tugevdus√µppesse](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> üé• Kl√µpsa √ºlaloleval pildil, et kuulata Dmitryt r√§√§kimas tugevdus√µppest

## [Eelloengu viktoriin](https://ff-quizzes.netlify.app/en/ml/)

## Eeltingimused ja seadistamine

Selles √µppet√ºkis katsetame m√µningaid Pythonis kirjutatud koode. Sa peaksid suutma k√§ivitada selle √µppet√ºki Jupyter Notebooki koodi kas oma arvutis v√µi pilves.

Sa saad avada [√µppet√ºki m√§rkmiku](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) ja j√§rgida seda √µppet√ºkki, et ehitada.

> **M√§rkus:** Kui avad selle koodi pilvest, pead alla laadima ka faili [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), mida kasutatakse m√§rkmiku koodis. Lisa see samasse kausta, kus m√§rkmik asub.

## Sissejuhatus

Selles √µppet√ºkis uurime **[Peeter ja hunt](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)** maailma, mis on inspireeritud vene helilooja [Sergei Prokofjevi](https://en.wikipedia.org/wiki/Sergei_Prokofiev) muusikalisest muinasjutust. Kasutame **tugevdus√µpet**, et lasta Peetril oma keskkonda avastada, maitsvaid √µunu korjata ja hundiga kohtumist v√§ltida.

**Tugevdus√µpe** (RL) on √µppetehnika, mis v√µimaldab meil √µppida **agendi** optimaalset k√§itumist mingis **keskkonnas**, viies l√§bi palju katseid. Agent peaks selles keskkonnas omama mingit **eesm√§rki**, mis on m√§√§ratletud **tasufunktsiooni** kaudu.

## Keskkond

Lihtsuse huvides kujutame ette, et Peetri maailm on ruudukujuline laud m√µ√µtmetega `laius` x `k√µrgus`, mis n√§eb v√§lja selline:

![Peetri keskkond](../../../../translated_images/environment.40ba3cb66256c93fa7e92f6f7214e1d1f588aafa97d266c11d108c5c5d101b6c.et.png)

Iga laua ruut v√µib olla:

* **maa**, millel Peeter ja teised olendid saavad k√µndida.
* **vesi**, millel ilmselgelt ei saa k√µndida.
* **puu** v√µi **rohi**, koht, kus saab puhata.
* **√µun**, mida Peeter oleks r√µ√µmus leida, et end toita.
* **hunt**, mis on ohtlik ja mida tuleks v√§ltida.

On olemas eraldi Python moodul, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), mis sisaldab koodi selle keskkonnaga t√∂√∂tamiseks. Kuna see kood ei ole meie kontseptsioonide m√µistmiseks oluline, impordime mooduli ja kasutame seda n√§idislaua loomiseks (koodil√µik 1):

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

See kood peaks printima keskkonna pildi, mis sarnaneb √ºlaltooduga.

## Tegevused ja poliitika

Meie n√§ites on Peetri eesm√§rk leida √µun, v√§ltides samal ajal hunti ja muid takistusi. Selleks saab ta p√µhim√µtteliselt ringi liikuda, kuni leiab √µuna.

Seega v√µib ta igas asukohas valida √ºhe j√§rgmistest tegevustest: √ºles, alla, vasakule ja paremale.

M√§√§ratleme need tegevused s√µnastikuna ja seome need vastavate koordinaatide muutustega. N√§iteks paremale liikumine (`R`) vastab paarile `(1,0)`. (koodil√µik 2):

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

Kokkuv√µtteks on selle stsenaariumi strateegia ja eesm√§rk j√§rgmised:

- **Strateegia**, meie agendi (Peetri) jaoks on m√§√§ratletud nn **poliitikaga**. Poliitika on funktsioon, mis tagastab tegevuse igas antud seisundis. Meie puhul on probleemi seisund esindatud laua ja m√§ngija praeguse asukohaga.

- **Eesm√§rk**, tugevdus√µppe eesm√§rk on l√µpuks √µppida hea poliitika, mis v√µimaldab meil probleemi t√µhusalt lahendada. Kuid alustuseks kaalume k√µige lihtsamat poliitikat, mida nimetatakse **juhuslikuks k√µndimiseks**.

## Juhuslik k√µndimine

Lahendame oma probleemi esmalt juhusliku k√µndimise strateegia abil. Juhusliku k√µndimise korral valime lubatud tegevuste hulgast juhuslikult j√§rgmise tegevuse, kuni j√µuame √µunani (koodil√µik 3).

1. Rakenda juhuslik k√µndimine alloleva koodiga:

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

   `walk`-funktsiooni v√§ljakutse peaks tagastama vastava tee pikkuse, mis v√µib √ºhest k√§ivituskorrast teise varieeruda.

1. K√§ivita k√µndimiskatse mitu korda (n√§iteks 100) ja prindi tulemuste statistika (koodil√µik 4):

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

   Pane t√§hele, et tee keskmine pikkus on umbes 30-40 sammu, mis on √ºsna palju, arvestades, et keskmine kaugus l√§hima √µunani on umbes 5-6 sammu.

   Samuti saad vaadata, kuidas Peetri liikumine juhusliku k√µndimise ajal v√§lja n√§eb:

   ![Peetri juhuslik k√µndimine](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## Tasufunktsioon

Selleks, et meie poliitika oleks intelligentsem, peame m√µistma, millised liigutused on "paremad" kui teised. Selleks peame m√§√§ratlema oma eesm√§rgi.

Eesm√§rki saab m√§√§ratleda **tasufunktsiooni** kaudu, mis tagastab iga seisundi kohta mingi punktisumma. Mida suurem on number, seda parem on tasufunktsioon. (koodil√µik 5)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

Huvitav on see, et tasufunktsioonide puhul antakse enamikul juhtudel *oluline tasu alles m√§ngu l√µpus*. See t√§hendab, et meie algoritm peaks kuidagi meelde j√§tma "head" sammud, mis viivad positiivse tasuni l√µpus, ja suurendama nende t√§htsust. Samamoodi tuleks k√µik liigutused, mis viivad halva tulemuseni, maha suruda.

## Q-√µpe

Algoritmi, mida siin arutame, nimetatakse **Q-√µppeks**. Selles algoritmis on poliitika m√§√§ratletud funktsiooni (v√µi andmestruktuuri) kaudu, mida nimetatakse **Q-tabeliks**. See salvestab iga tegevuse "headuse" antud seisundis.

Seda nimetatakse Q-tabeliks, kuna seda on sageli mugav esitada tabelina v√µi mitmem√µ√µtmelise massiivina. Kuna meie laual on m√µ√µtmed `laius` x `k√µrgus`, saame Q-tabelit esitada numpy massiivina kujuga `laius` x `k√µrgus` x `len(actions)`: (koodil√µik 6)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

Pange t√§hele, et algv√§√§rtustame k√µik Q-tabeli v√§√§rtused v√µrdse v√§√§rtusega, meie puhul - 0.25. See vastab "juhusliku k√µndimise" poliitikale, kuna k√µik liigutused igas seisundis on v√µrdselt head. Saame Q-tabeli edastada `plot` funktsioonile, et visualiseerida tabelit laual: `m.plot(Q)`.

![Peetri keskkond](../../../../translated_images/env_init.04e8f26d2d60089e128f21d22e5fef57d580e559f0d5937b06c689e5e7cdd438.et.png)

Iga ruudu keskel on "nooleke", mis n√§itab eelistatud liikumissuunda. Kuna k√µik suunad on v√µrdsed, kuvatakse punkt.

N√º√ºd peame k√§ivitama simulatsiooni, uurima oma keskkonda ja √µppima paremat Q-tabeli v√§√§rtuste jaotust, mis v√µimaldab meil √µunani j√µuda palju kiiremini.

## Q-√µppe olemus: Bellmani v√µrrand

Kui hakkame liikuma, on igal tegevusel vastav tasu, st me saame teoreetiliselt valida j√§rgmise tegevuse, l√§htudes k√µrgeimast kohesest tasust. Kuid enamikus seisundites ei saavuta liigutus meie eesm√§rki j√µuda √µunani, mist√µttu ei saa me kohe otsustada, milline suund on parem.

> Pea meeles, et oluline pole kohene tulemus, vaid l√µpptulemus, mille saame simulatsiooni l√µpus.

Selleks, et arvestada viivitusega tasu, peame kasutama **[d√ºnaamilise programmeerimise](https://en.wikipedia.org/wiki/Dynamic_programming)** p√µhim√µtteid, mis v√µimaldavad meil probleemi k√§sitleda rekursiivselt.

Oletame, et oleme n√º√ºd seisundis *s* ja tahame liikuda j√§rgmisesse seisundisse *s'*. Seda tehes saame kohese tasu *r(s,a)*, mis on m√§√§ratletud tasufunktsiooni kaudu, pluss mingi tulevane tasu. Kui eeldame, et meie Q-tabel kajastab √µigesti iga tegevuse "atraktiivsust", siis seisundis *s'* valime tegevuse *a*, mis vastab maksimaalsele v√§√§rtusele *Q(s',a')*. Seega on parim v√µimalik tulevane tasu, mida me seisundis *s* saame, m√§√§ratletud kui `max`<sub>a'</sub>*Q(s',a')* (maksimum arvutatakse siin k√µigi v√µimalike tegevuste *a'* √ºle seisundis *s'*).

See annab **Bellmani valemi**, mille abil arvutada Q-tabeli v√§√§rtust seisundis *s*, arvestades tegevust *a*:

<img src="../../../../translated_images/bellman-equation.7c0c4c722e5a6b7c208071a0bae51664965050848e4f8a84bb377cd18bdd838b.et.png"/>

Siin Œ≥ on nn **diskonteerimistegur**, mis m√§√§rab, mil m√§√§ral peaks eelistama praegust tasu tulevase tasu ees ja vastupidi.

## √ïppealgoritm

Eeltoodud valemi p√µhjal saame n√º√ºd kirjutada oma √µppealgoritmi pseudokoodi:

* Algv√§√§rtusta Q-tabel Q v√µrdsete numbritega k√µigi seisundite ja tegevuste jaoks
* M√§√§ra √µppem√§√§r Œ± ‚Üê 1
* Korda simulatsiooni mitu korda
   1. Alusta juhuslikust asukohast
   1. Korda
        1. Vali tegevus *a* seisundis *s*
        2. Soorita tegevus, liikudes uude seisundisse *s'*
        3. Kui kohtame m√§ngu l√µpu tingimust v√µi kogutud tasu on liiga v√§ike - l√µpeta simulatsioon  
        4. Arvuta tasu *r* uues seisundis
        5. Uuenda Q-funktsiooni vastavalt Bellmani valemile: *Q(s,a)* ‚Üê *(1-Œ±)Q(s,a)+Œ±(r+Œ≥ max<sub>a'</sub>Q(s',a'))*
        6. *s* ‚Üê *s'*
        7. Uuenda kogutud tasu ja v√§henda Œ±.

## Kasutamine vs. avastamine

√úlaltoodud algoritmis ei t√§psustanud me, kuidas t√§pselt peaksime valima tegevuse sammus 2.1. Kui valime tegevuse juhuslikult, **avastame** me keskkonda juhuslikult ja t√µen√§oliselt sureme sageli ning uurime ka piirkondi, kuhu me tavaliselt ei l√§heks. Alternatiivne l√§henemine oleks **kasutada** Q-tabeli v√§√§rtusi, mida me juba teame, ja valida seega parim tegevus (k√µrgema Q-tabeli v√§√§rtusega) seisundis *s*. See aga takistab meil uurida teisi seisundeid ja t√µen√§oliselt ei leia me optimaalset lahendust.

Seega on parim l√§henemine leida tasakaal avastamise ja kasutamise vahel. Seda saab teha, valides tegevuse seisundis *s* t√µen√§osustega, mis on proportsionaalsed Q-tabeli v√§√§rtustega. Alguses, kui Q-tabeli v√§√§rtused on k√µik √ºhesugused, vastab see juhuslikule valikule, kuid mida rohkem me oma keskkonna kohta √µpime, seda t√µen√§olisemalt j√§rgime optimaalset teed, lubades agendil aeg-ajalt valida ka uurimata tee.

## Pythoni implementatsioon

N√º√ºd oleme valmis √µppealgoritmi rakendama. Enne seda vajame ka m√µnda funktsiooni, mis teisendab Q-tabeli suvalised numbrid vastavate tegevuste t√µen√§osusvektoriks.

1. Loo funktsioon `probs()`:

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    ```

   Lisame algsele vektorile m√µned `eps`, et v√§ltida jagamist nulliga algses olukorras, kui k√µik vektori komponendid on identsed.

K√§ivita √µppealgoritm l√§bi 5000 eksperimendi, mida nimetatakse ka **epohhideks**: (koodil√µik 8)
```python
    for epoch in range(5000):
    
        # Pick initial point
        m.random_start()
        
        # Start travelling
        n=0
        cum_reward = 0
        while True:
            x,y = m.human
            v = probs(Q[x,y])
            a = random.choices(list(actions),weights=v)[0]
            dpos = actions[a]
            m.move(dpos,check_correctness=False) # we allow player to move outside the board, which terminates episode
            r = reward(m)
            cum_reward += r
            if r==end_reward or cum_reward < -1000:
                lpath.append(n)
                break
            alpha = np.exp(-n / 10e5)
            gamma = 0.5
            ai = action_idx[a]
            Q[x,y,ai] = (1 - alpha) * Q[x,y,ai] + alpha * (r + gamma * Q[x+dpos[0], y+dpos[1]].max())
            n+=1
```

P√§rast selle algoritmi t√§itmist peaks Q-tabel olema uuendatud v√§√§rtustega, mis m√§√§ratlevad erinevate tegevuste atraktiivsuse igas etapis. Saame proovida Q-tabelit visualiseerida, joonistades igasse ruutu vektori, mis osutab soovitud liikumissuunda. Lihtsuse huvides joonistame noolepea asemel v√§ikese ringi.

<img src="../../../../translated_images/learned.ed28bcd8484b5287a31925c96c43b43e2c2bb876b8ca41a0e1e754f77bb3db20.et.png"/>

## Poliitika kontrollimine

Kuna Q-tabel loetleb iga tegevuse "atraktiivsuse" igas seisundis, on seda √ºsna lihtne kasutada t√µhusa navigeerimise m√§√§ratlemiseks meie maailmas. Lihtsaimas variandis saame valida tegevuse, mis vastab k√µrgeimale Q-tabeli v√§√§rtusele: (koodil√µik 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Kui proovite √ºlaltoodud koodi mitu korda, v√µite m√§rgata, et see m√µnikord "hangub" ja peate vajutama m√§rkmikus STOP-nuppu, et see katkestada. See juhtub, kuna v√µivad tekkida olukorrad, kus kaks olekut "osutavad" √ºksteisele optimaalse Q-v√§√§rtuse osas, mille tulemusel liigub agent nende olekute vahel l√µputult edasi-tagasi.

## üöÄV√§ljakutse

> **√úlesanne 1:** Muutke funktsiooni `walk`, et piirata maksimaalset teekonna pikkust teatud arvu sammudega (n√§iteks 100) ja vaadake, kuidas √ºlaltoodud kood aeg-ajalt selle v√§√§rtuse tagastab.

> **√úlesanne 2:** Muutke funktsiooni `walk`, et see ei l√§heks tagasi kohtadesse, kus ta on juba varem olnud. See takistab `walk` funktsiooni ts√ºklisse j√§√§mist, kuid agent v√µib siiski sattuda olukorda, kus ta ei suuda asukohast v√§lja p√§√§seda.

## Navigeerimine

Parem navigeerimispoliitika oleks see, mida kasutasime treeningu ajal, mis √ºhendab ekspluateerimise ja uurimise. Selle poliitika puhul valime iga tegevuse teatud t√µen√§osusega, mis on proportsionaalne Q-tabeli v√§√§rtustega. See strateegia v√µib siiski viia agendi tagasi positsioonile, mida ta on juba uurinud, kuid nagu n√§ete allolevast koodist, viib see v√§ga l√ºhikese keskmise teekonna pikkuseni soovitud asukohta (pidage meeles, et `print_statistics` k√§ivitab simulatsiooni 100 korda): (koodiplokk 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

P√§rast selle koodi k√§ivitamist peaksite saama palju v√§iksema keskmise teekonna pikkuse kui varem, vahemikus 3‚Äì6.

## √ïppimisprotsessi uurimine

Nagu mainitud, on √µppimisprotsess tasakaal uurimise ja olemasoleva teadmise rakendamise vahel probleemiruumi struktuuri kohta. Oleme n√§inud, et √µppimise tulemused (v√µime aidata agenti leida l√ºhike tee eesm√§rgini) on paranenud, kuid huvitav on ka j√§lgida, kuidas keskmine teekonna pikkus k√§itub √µppimisprotsessi ajal:

<img src="../../../../translated_images/lpathlen1.0534784add58d4ebf25c21d4a1da9bceab4f96743a35817f1b49ab963c64c572.et.png"/>

√ïppimisprotsessi saab kokku v√µtta j√§rgmiselt:

- **Keskmine teekonna pikkus suureneb**. Alguses suureneb keskmine teekonna pikkus. T√µen√§oliselt on see tingitud asjaolust, et kui me ei tea keskkonnast midagi, j√§√§me t√µen√§oliselt kinni halbadest olekutest, nagu vesi v√µi hunt. Kui √µpime rohkem ja hakkame seda teadmist kasutama, saame keskkonda kauem uurida, kuid me ei tea veel h√§sti, kus √µunad asuvad.

- **Teekonna pikkus v√§heneb, kui √µpime rohkem**. Kui oleme piisavalt √µppinud, muutub agendil eesm√§rgi saavutamine lihtsamaks ja teekonna pikkus hakkab v√§henema. Kuid oleme endiselt avatud uurimisele, mist√µttu kaldume sageli parimast teest k√µrvale ja uurime uusi v√µimalusi, muutes teekonna pikemaks kui optimaalne.

- **Pikkus suureneb j√§rsult**. Graafikul n√§eme ka, et mingil hetkel pikkus suurenes j√§rsult. See n√§itab protsessi juhuslikku olemust ja seda, et v√µime mingil hetkel "rikkuda" Q-tabeli koefitsiendid, kirjutades neile uued v√§√§rtused √ºle. Seda tuleks ideaalis minimeerida √µppem√§√§ra v√§hendamisega (n√§iteks treeningu l√µpus kohandame Q-tabeli v√§√§rtusi ainult v√§ikese v√§√§rtusega).

√úldiselt on oluline meeles pidada, et √µppimisprotsessi edu ja kvaliteet s√µltuvad oluliselt parameetritest, nagu √µppem√§√§r, √µppem√§√§ra kahanemine ja diskontom√§√§r. Neid nimetatakse sageli **h√ºperparameetriteks**, et eristada neid **parameetritest**, mida optimeerime treeningu ajal (n√§iteks Q-tabeli koefitsiendid). Parimate h√ºperparameetrite v√§√§rtuste leidmise protsessi nimetatakse **h√ºperparameetrite optimeerimiseks** ja see v√§√§rib eraldi teemat.

## [Loengu j√§rgne viktoriin](https://ff-quizzes.netlify.app/en/ml/)

## √úlesanne 
[Realistlikum maailm](assignment.md)

---

**Lahti√ºtlus**:  
See dokument on t√µlgitud AI t√µlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi p√º√ºame tagada t√§psust, palume arvestada, et automaatsed t√µlked v√µivad sisaldada vigu v√µi ebat√§psusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimt√µlget. Me ei vastuta selle t√µlke kasutamisest tulenevate arusaamatuste v√µi valesti t√µlgenduste eest.