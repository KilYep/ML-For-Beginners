<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7cdd17338d9bbd7e2171c2cd462eb081",
  "translation_date": "2025-10-11T12:08:02+00:00",
  "source_file": "5-Clustering/2-K-Means/README.md",
  "language_code": "et"
}
-->
# K-Means klasterdamine

## [Eelloengu viktoriin](https://ff-quizzes.netlify.app/en/ml/)

Selles ÃµppetÃ¼kis Ãµpid, kuidas luua klastreid, kasutades Scikit-learn'i ja varem imporditud Nigeeria muusika andmestikku. KÃ¤sitleme K-Meansi klasterdamise pÃµhitÃµdesid. Pea meeles, et nagu eelnevas ÃµppetÃ¼kis Ãµppisid, on klastritega tÃ¶Ã¶tamiseks palju erinevaid meetodeid ja valik sÃµltub sinu andmetest. Proovime K-Meansi, kuna see on kÃµige levinum klasterdamise tehnika. Alustame!

MÃµisted, mida Ãµpid:

- Silueti skoor
- KÃ¼Ã¼narnuki meetod
- Inerts
- Variants

## Sissejuhatus

[K-Meansi klasterdamine](https://wikipedia.org/wiki/K-means_clustering) on meetod, mis pÃ¤rineb signaalitÃ¶Ã¶tluse valdkonnast. Seda kasutatakse andmete jagamiseks ja rÃ¼hmitamiseks 'k' klastritesse, kasutades vaatluste seeriat. Iga vaatlus tÃ¶Ã¶tab selle nimel, et rÃ¼hmitada antud andmepunkt lÃ¤hima 'keskmise' ehk klastri keskpunkti juurde.

Klastreid saab visualiseerida kui [Voronoi diagramme](https://wikipedia.org/wiki/Voronoi_diagram), mis sisaldavad punkti (vÃµi 'seemet') ja selle vastavat piirkonda.

![voronoi diagramm](../../../../translated_images/voronoi.1dc1613fb0439b9564615eca8df47a4bcd1ce06217e7e72325d2406ef2180795.et.png)

> infograafik autorilt [Jen Looper](https://twitter.com/jenlooper)

K-Meansi klasterdamise protsess [toimub kolmes etapis](https://scikit-learn.org/stable/modules/clustering.html#k-means):

1. Algoritm valib k-arvu keskpunktid, vÃµttes proove andmestikust. SeejÃ¤rel kordab:
    1. MÃ¤Ã¤rab iga proovi lÃ¤hima centroidi juurde.
    2. Loob uued centroidid, arvutades kÃµigi eelmiste centroidide juurde mÃ¤Ã¤ratud proovide keskmise vÃ¤Ã¤rtuse.
    3. Arvutab uute ja vanade centroidide erinevuse ning kordab, kuni centroidid stabiliseeruvad.

Ãœks K-Meansi kasutamise puudus on see, et pead mÃ¤Ã¤rama 'k', ehk centroidide arvu. Ã•nneks aitab 'kÃ¼Ã¼narnuki meetod' hinnata head algvÃ¤Ã¤rtust 'k' jaoks. Proovime seda kohe.

## Eeltingimus

TÃ¶Ã¶tad selle ÃµppetÃ¼ki [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/2-K-Means/notebook.ipynb) failis, mis sisaldab andmete importimist ja esmast puhastamist, mida tegid eelmises ÃµppetÃ¼kis.

## Harjutus - ettevalmistus

Alusta, vaadates uuesti laulude andmeid.

1. Loo kastdiagramm, kutsudes `boxplot()` iga veeru jaoks:

    ```python
    plt.figure(figsize=(20,20), dpi=200)
    
    plt.subplot(4,3,1)
    sns.boxplot(x = 'popularity', data = df)
    
    plt.subplot(4,3,2)
    sns.boxplot(x = 'acousticness', data = df)
    
    plt.subplot(4,3,3)
    sns.boxplot(x = 'energy', data = df)
    
    plt.subplot(4,3,4)
    sns.boxplot(x = 'instrumentalness', data = df)
    
    plt.subplot(4,3,5)
    sns.boxplot(x = 'liveness', data = df)
    
    plt.subplot(4,3,6)
    sns.boxplot(x = 'loudness', data = df)
    
    plt.subplot(4,3,7)
    sns.boxplot(x = 'speechiness', data = df)
    
    plt.subplot(4,3,8)
    sns.boxplot(x = 'tempo', data = df)
    
    plt.subplot(4,3,9)
    sns.boxplot(x = 'time_signature', data = df)
    
    plt.subplot(4,3,10)
    sns.boxplot(x = 'danceability', data = df)
    
    plt.subplot(4,3,11)
    sns.boxplot(x = 'length', data = df)
    
    plt.subplot(4,3,12)
    sns.boxplot(x = 'release_date', data = df)
    ```

    Need andmed on veidi mÃ¼ra tÃ¤is: iga veeru kastdiagrammi vaadates nÃ¤ed kÃµrvalekaldeid.

    ![kÃµrvalekalded](../../../../translated_images/boxplots.8228c29dabd0f29227dd38624231a175f411f1d8d4d7c012cb770e00e4fdf8b6.et.png)

Sa vÃµiksid andmestiku lÃ¤bi kÃ¤ia ja need kÃµrvalekalded eemaldada, kuid see muudaks andmed Ã¼sna minimaalseks.

1. Praegu vali, milliseid veerge kasutad klasterdamise harjutuseks. Vali need, millel on sarnased vahemikud, ja kodeeri veerg `artist_top_genre` numbrilisteks andmeteks:

    ```python
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    
    X = df.loc[:, ('artist_top_genre','popularity','danceability','acousticness','loudness','energy')]
    
    y = df['artist_top_genre']
    
    X['artist_top_genre'] = le.fit_transform(X['artist_top_genre'])
    
    y = le.transform(y)
    ```

1. NÃ¼Ã¼d pead valima, kui palju klastreid sihtida. Tead, et andmestikust eraldasime 3 laulude Å¾anrit, seega proovime 3:

    ```python
    from sklearn.cluster import KMeans
    
    nclusters = 3 
    seed = 0
    
    km = KMeans(n_clusters=nclusters, random_state=seed)
    km.fit(X)
    
    # Predict the cluster for each data point
    
    y_cluster_kmeans = km.predict(X)
    y_cluster_kmeans
    ```

NÃ¤ed vÃ¤lja prinditud massiivi, kus iga andmeraami rea jaoks on ennustatud klaster (0, 1 vÃµi 2).

1. Kasuta seda massiivi, et arvutada 'silueti skoor':

    ```python
    from sklearn import metrics
    score = metrics.silhouette_score(X, y_cluster_kmeans)
    score
    ```

## Silueti skoor

Otsi silueti skoori, mis on lÃ¤hemal 1-le. See skoor varieerub -1-st 1-ni, ja kui skoor on 1, on klaster tihe ja hÃ¤sti eraldatud teistest klastritest. VÃ¤Ã¤rtus, mis on lÃ¤hedal 0-le, tÃ¤histab kattuvaid klastreid, kus proovid on vÃ¤ga lÃ¤hedal naaberklastrite otsustuspiirile. [(Allikas)](https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam)

Meie skoor on **.53**, seega keskmine. See nÃ¤itab, et meie andmed ei sobi eriti hÃ¤sti selliseks klasterdamiseks, kuid jÃ¤tkame.

### Harjutus - mudeli loomine

1. Impordi `KMeans` ja alusta klasterdamise protsessi.

    ```python
    from sklearn.cluster import KMeans
    wcss = []
    
    for i in range(1, 11):
        kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
        kmeans.fit(X)
        wcss.append(kmeans.inertia_)
    
    ```

    Siin on mÃµned osad, mis vajavad selgitamist.

    > ðŸŽ“ range: Need on klasterdamise protsessi iteratsioonid.

    > ðŸŽ“ random_state: "MÃ¤Ã¤rab juhuslike arvude genereerimise centroidide algvÃ¤Ã¤rtustamiseks." [Allikas](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)

    > ðŸŽ“ WCSS: "klastri sees olevate ruutude summa" mÃµÃµdab kÃµigi punktide keskmist ruutkaugust klastri centroidist. [Allikas](https://medium.com/@ODSC/unsupervised-learning-evaluating-clusters-bd47eed175ce).

    > ðŸŽ“ Inerts: K-Meansi algoritmid pÃ¼Ã¼avad valida centroidid, et minimeerida 'inertsust', "mÃµÃµdet, kui sisemiselt koherentne klaster on." [Allikas](https://scikit-learn.org/stable/modules/clustering.html). VÃ¤Ã¤rtus lisatakse wcss muutujale igal iteratsioonil.

    > ðŸŽ“ k-means++: [Scikit-learn'is](https://scikit-learn.org/stable/modules/clustering.html#k-means) saad kasutada 'k-means++' optimeerimist, mis "algvÃ¤Ã¤rtustab centroidid, et need oleksid (Ã¼ldiselt) Ã¼ksteisest kaugel, mis viib tÃµenÃ¤oliselt paremate tulemusteni kui juhuslik algvÃ¤Ã¤rtustamine."

### KÃ¼Ã¼narnuki meetod

Varem arvasid, et kuna sihtisid 3 laulude Å¾anrit, peaksid valima 3 klastrit. Aga kas see on nii?

1. Kasuta 'kÃ¼Ã¼narnuki meetodit', et olla kindel.

    ```python
    plt.figure(figsize=(10,5))
    sns.lineplot(x=range(1, 11), y=wcss, marker='o', color='red')
    plt.title('Elbow')
    plt.xlabel('Number of clusters')
    plt.ylabel('WCSS')
    plt.show()
    ```

    Kasuta `wcss` muutujat, mille ehitasid eelmises etapis, et luua diagramm, mis nÃ¤itab, kus on 'kÃ¼Ã¼narnuki' painutus, mis nÃ¤itab optimaalset klastrite arvu. VÃµib-olla on see tÃµesti **3**!

    ![kÃ¼Ã¼narnuki meetod](../../../../translated_images/elbow.72676169eed744ff03677e71334a16c6b8f751e9e716e3d7f40dd7cdef674cca.et.png)

## Harjutus - klastrite kuvamine

1. Proovi protsessi uuesti, seekord mÃ¤Ã¤rates kolm klastrit, ja kuva klastrid hajusdiagrammina:

    ```python
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters = 3)
    kmeans.fit(X)
    labels = kmeans.predict(X)
    plt.scatter(df['popularity'],df['danceability'],c = labels)
    plt.xlabel('popularity')
    plt.ylabel('danceability')
    plt.show()
    ```

1. Kontrolli mudeli tÃ¤psust:

    ```python
    labels = kmeans.labels_
    
    correct_labels = sum(y == labels)
    
    print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))
    
    print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))
    ```

    Selle mudeli tÃ¤psus ei ole vÃ¤ga hea ja klastrite kuju annab vihje, miks.

    ![klastrid](../../../../translated_images/clusters.b635354640d8e4fd4a49ef545495518e7be76172c97c13bd748f5b79f171f69a.et.png)

    Need andmed on liiga tasakaalust vÃ¤ljas, liiga vÃ¤he korrelatsioonis ja veergude vÃ¤Ã¤rtuste vahel on liiga palju variatsiooni, et hÃ¤sti klasterdada. Tegelikult on klastrid, mis moodustuvad, tÃµenÃ¤oliselt tugevalt mÃµjutatud vÃµi kallutatud kolme Å¾anrikategooria poolt, mille me Ã¼lal mÃ¤Ã¤ratlesime. See oli Ãµppeprotsess!

    Scikit-learn'i dokumentatsioonis nÃ¤ed, et mudel nagu see, kus klastrid ei ole vÃ¤ga hÃ¤sti eraldatud, on 'variantsi' probleemiga:

    ![probleemsed mudelid](../../../../translated_images/problems.f7fb539ccd80608e1f35c319cf5e3ad1809faa3c08537aead8018c6b5ba2e33a.et.png)
    > Infograafik Scikit-learn'ist

## Variants

Variants on defineeritud kui "keskmine ruutude erinevustest keskmisest" [(Allikas)](https://www.mathsisfun.com/data/standard-deviation.html). Selle klasterdamisprobleemi kontekstis viitab see andmetele, kus meie andmestiku numbrid kipuvad keskmisest liiga palju kÃµrvale kalduma.

âœ… See on suurepÃ¤rane hetk mÃµelda kÃµigile viisidele, kuidas saaksid seda probleemi lahendada. Kas andmeid veidi rohkem kohandada? Kasutada erinevaid veerge? Kasutada teistsugust algoritmi? Vihje: Proovi [andmete skaleerimist](https://www.mygreatlearning.com/blog/learning-data-science-with-k-means-clustering/), et neid normaliseerida ja testida teisi veerge.

> Proovi seda '[variantsi kalkulaatorit](https://www.calculatorsoup.com/calculators/statistics/variance-calculator.php)', et mÃµista kontseptsiooni veidi paremini.

---

## ðŸš€VÃ¤ljakutse

Veeda aega selle mÃ¤rkmikuga, kohandades parameetreid. Kas suudad mudeli tÃ¤psust parandada, puhastades andmeid rohkem (nÃ¤iteks eemaldades kÃµrvalekaldeid)? Saad kasutada kaalu, et anda teatud andmeproovidele rohkem kaalu. Mida veel saad teha, et luua paremaid klastreid?

Vihje: Proovi oma andmeid skaleerida. MÃ¤rkmikus on kommenteeritud kood, mis lisab standardse skaleerimise, et muuta andmeveerud vahemiku osas Ã¼ksteisele sarnasemaks. NÃ¤ed, et kuigi silueti skoor langeb, muutub kÃ¼Ã¼narnuki graafiku 'kink' sujuvamaks. See on tingitud sellest, et andmete skaleerimata jÃ¤tmine vÃµimaldab vÃ¤iksema variatsiooniga andmetel rohkem kaalu kanda. Loe selle probleemi kohta veidi rohkem [siin](https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clustering/21226#21226).

## [JÃ¤rel-loengu viktoriin](https://ff-quizzes.netlify.app/en/ml/)

## Ãœlevaade ja iseseisev Ãµpe

Vaata K-Meansi simulaatorit [nÃ¤iteks seda](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/). Saad kasutada seda tÃ¶Ã¶riista, et visualiseerida nÃ¤idisandmepunkte ja mÃ¤Ã¤rata nende centroidid. Saad muuta andmete juhuslikkust, klastrite arvu ja centroidide arvu. Kas see aitab sul paremini mÃµista, kuidas andmeid saab rÃ¼hmitada?

Samuti vaata [seda K-Meansi kÃ¤siraamatut](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html) Stanfordist.

## Ãœlesanne

[Proovi erinevaid klasterdamise meetodeid](assignment.md)

---

**Vastutusest loobumine**:  
See dokument on tÃµlgitud AI tÃµlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi pÃ¼Ã¼ame tagada tÃ¤psust, palume arvestada, et automaatsed tÃµlked vÃµivad sisaldada vigu vÃµi ebatÃ¤psusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimtÃµlget. Me ei vastuta selle tÃµlke kasutamisest tulenevate arusaamatuste vÃµi valesti tÃµlgenduste eest.