<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1f2b7441745eb52e25745423b247016b",
  "translation_date": "2025-10-11T11:17:31+00:00",
  "source_file": "8-Reinforcement/2-Gym/assignment.md",
  "language_code": "ta"
}
-->
# மலை கார் பயிற்சி

[OpenAI Gym](http://gym.openai.com) இவ்வாறு வடிவமைக்கப்பட்டுள்ளது, அனைத்து சூழல்களும் ஒரே API-ஐ வழங்குகின்றன - அதாவது, ஒரே `reset`, `step` மற்றும் `render` முறைமைகள் மற்றும் **action space** மற்றும் **observation space** ஆகியவற்றின் ஒரே சுருக்கங்கள். எனவே, reinforcement learning algorithm-களை குறைந்த அளவு குறியீட்டு மாற்றங்களுடன் வெவ்வேறு சூழல்களுக்கு ஏற்றுக்கொள்ள முடியும்.

## மலை கார் சூழல்

[மலை கார் சூழல்](https://gym.openai.com/envs/MountainCar-v0/) ஒரு பள்ளத்தாக்கில் சிக்கியுள்ள காரை கொண்டுள்ளது:

<img src="../../../../translated_images/mountaincar.43d56e588ce581c2d035f28cf038a9af112bec043b2ef8da40ac86119b1e3a93.ta.png" width="300"/>

பள்ளத்தாக்கிலிருந்து வெளியேறி கொடியை பிடிக்க வேண்டும், இதற்காக ஒவ்வொரு படியிலும் பின்வரும் செயல்களில் ஒன்றை செய்ய வேண்டும்:

| மதிப்பு | பொருள் |
|---|---|
| 0 | இடதுபுறம் வேகத்தை அதிகரிக்கவும் |
| 1 | வேகத்தை அதிகரிக்க வேண்டாம் |
| 2 | வலதுபுறம் வேகத்தை அதிகரிக்கவும் |

இந்த பிரச்சனையின் முக்கிய தந்திரம் என்னவென்றால், காரின் இயந்திரம் ஒரு முறை முயற்சியில் மலைக்கு ஏறுவதற்கு போதுமான சக்தி கொண்டது இல்லை. எனவே, வெற்றி பெற ஒரே வழி, வேகத்தை அதிகரிக்க பின்சென்று மீண்டும் முன்னேற வேண்டும்.

Observation space இரண்டு மதிப்புகளை மட்டுமே கொண்டுள்ளது:

| எண் | கண்காணிப்பு | குறைந்தபட்சம் | அதிகபட்சம் |
|-----|--------------|-----|-----|
|  0  | கார் நிலை | -1.2 | 0.6 |
|  1  | கார் வேகம் | -0.07 | 0.07 |

மலை கார் பரிசு அமைப்பு மிகவும் சிக்கலானது:

 * மலை உச்சியில் கொடியை (position = 0.5) அடைந்தால், 0 பரிசு வழங்கப்படும்.
 * முகவர் 0.5 க்கு குறைவான நிலையை அடைந்தால், -1 பரிசு வழங்கப்படும்.

கார் நிலை 0.5 க்கு மேல் இருந்தால் அல்லது எபிசோட் நீளம் 200 க்கு மேல் இருந்தால் எபிசோட் முடிவடைகிறது.

## வழிமுறைகள்

மலை கார் பிரச்சனையை தீர்க்க எங்கள் reinforcement learning algorithm-ஐ ஏற்றுக்கொள்ளவும். உள்ள [notebook.ipynb](notebook.ipynb) குறியீட்டுடன் தொடங்கவும், புதிய சூழலை மாற்றவும், state discretization செயல்பாடுகளை மாற்றவும், மற்றும் குறைந்த அளவு குறியீட்டு மாற்றங்களுடன் உள்ள algorithm-ஐ பயிற்சி செய்ய முயற்சிக்கவும். Hyperparameters-ஐ சரிசெய்து முடிவுகளை மேம்படுத்தவும்.

> **குறிப்பு**: Algorithm-ஐ சரியாக செயல்பட Hyperparameters-ஐ சரிசெய்தல் தேவைப்படும். 

## மதிப்பீடு

| அளவுகோல் | சிறந்தது | போதுமானது | மேம்பாடு தேவை |
| -------- | --------- | -------- | ----------------- |
|          | CartPole எடுத்துக்காட்டிலிருந்து Q-Learning algorithm வெற்றிகரமாக ஏற்றுக்கொள்ளப்பட்டுள்ளது, குறைந்த அளவு குறியீட்டு மாற்றங்களுடன், 200 படிகளில் கொடியை பிடிக்கும் பிரச்சனையை தீர்க்க முடியும். | புதிய Q-Learning algorithm இணையத்திலிருந்து ஏற்றுக்கொள்ளப்பட்டுள்ளது, ஆனால் நன்கு ஆவணப்படுத்தப்பட்டுள்ளது; அல்லது உள்ள algorithm ஏற்றுக்கொள்ளப்பட்டுள்ளது, ஆனால் விரும்பிய முடிவுகளை அடையவில்லை | மாணவர் எந்த algorithm-ஐயும் வெற்றிகரமாக ஏற்றுக்கொள்ள முடியவில்லை, ஆனால் தீர்வை நோக்கி முக்கியமான நடவடிக்கைகளை எடுத்துள்ளார் (state discretization, Q-Table தரவமைப்பு, முதலியன செயல்படுத்தியுள்ளார்) |

---

**அறிவிப்பு**:  
இந்த ஆவணம் [Co-op Translator](https://github.com/Azure/co-op-translator) என்ற AI மொழிபெயர்ப்பு சேவையை பயன்படுத்தி மொழிபெயர்க்கப்பட்டுள்ளது. நாங்கள் துல்லியத்திற்காக முயற்சிக்கிறோம், ஆனால் தானியங்கி மொழிபெயர்ப்புகளில் பிழைகள் அல்லது தவறுகள் இருக்கக்கூடும் என்பதை கவனத்தில் கொள்ளவும். அதன் சொந்த மொழியில் உள்ள மூல ஆவணம் அதிகாரப்பூர்வ ஆதாரமாக கருதப்பட வேண்டும். முக்கியமான தகவல்களுக்கு, தொழில்முறை மனித மொழிபெயர்ப்பு பரிந்துரைக்கப்படுகிறது. இந்த மொழிபெயர்ப்பைப் பயன்படுத்துவதால் ஏற்படும் எந்த தவறான புரிதல்களுக்கும் அல்லது தவறான விளக்கங்களுக்கும் நாங்கள் பொறுப்பல்ல.