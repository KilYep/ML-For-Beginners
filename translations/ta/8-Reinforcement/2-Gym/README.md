<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "107d5bb29da8a562e7ae72262d251a75",
  "translation_date": "2025-10-11T11:15:27+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "ta"
}
-->
# கார்ட்போல் ஸ்கேட்டிங்

முந்தைய பாடத்தில் நாம் தீர்க்க முயன்ற பிரச்சினை ஒரு விளையாட்டு பிரச்சினையாக தோன்றலாம், இது உண்மையான வாழ்க்கை சூழல்களுக்கு பொருந்தாது. ஆனால் இது உண்மையல்ல, ஏனெனில் பல உண்மையான உலக பிரச்சினைகளும் இந்த சூழலுடன் பகிர்ந்து கொள்ளுகின்றன - சதுரங்கம் அல்லது கோவோடு விளையாடுவது உட்பட. அவை ஒரே மாதிரியானவை, ஏனெனில் நமக்கு கொடுக்கப்பட்ட விதிகளுடன் ஒரு பலகை மற்றும் **தொகுக்கப்பட்ட நிலை** உள்ளது.

## [பாடத்திற்கு முன் வினாடி வினா](https://ff-quizzes.netlify.app/en/ml/)

## அறிமுகம்

இந்த பாடத்தில், Q-Learning இன் அதே கொள்கைகளை **தொடர்ச்சியான நிலை** கொண்ட ஒரு பிரச்சினைக்கு பயன்படுத்துவோம், அதாவது ஒரு அல்லது அதற்கு மேற்பட்ட உண்மையான எண்களால் கொடுக்கப்பட்ட நிலை. நாம் பின்வரும் பிரச்சினையை கையாளுவோம்:

> **பிரச்சினை**: பீட்டர் ஓநாயிடம் இருந்து தப்பிக்க விரும்பினால், அவர் வேகமாக நகர முடியும் திறன் பெற வேண்டும். பீட்டர் ஸ்கேட் செய்ய கற்றுக்கொள்வது, குறிப்பாக சமநிலையை பராமரிப்பது எப்படி என்பதை நாம் காண்போம்.

![பெரிய தப்பிப்பு!](../../../../translated_images/escape.18862db9930337e3fce23a9b6a76a06445f229dadea2268e12a6f0a1fde12115.ta.png)

> பீட்டர் மற்றும் அவரது நண்பர்கள் ஓநாயிடம் இருந்து தப்பிக்க சிருஷ்டிப்பாக செயல்படுகிறார்கள்! படம்: [ஜென் லூப்பர்](https://twitter.com/jenlooper)

நாம் **CartPole** பிரச்சினை என அறியப்படும் சமநிலையை பராமரிக்கும் ஒரு எளிமையான பதிப்பைப் பயன்படுத்துவோம். கார்ட்போல் உலகில், இடது அல்லது வலது நோக்கி நகரக்கூடிய ஒரு தூரிகை உள்ளது, மேலும் நோக்கம் தூரிகையின் மேல் உள்ள செங்குத்து கம்பத்தை சமநிலைப்படுத்துவது.

<img alt="ஒரு கார்ட்போல்" src="../../../../translated_images/cartpole.b5609cc0494a14f75d121299495ae24fd8f1c30465e7b40961af94ecda2e1cd0.ta.png" width="200"/>

## முன்னோட்டம்

இந்த பாடத்தில், **OpenAI Gym** எனப்படும் ஒரு நூலகத்தை பயன்படுத்தி பல்வேறு **சூழல்களை** சிமுலேட் செய்வோம். இந்த பாடத்தின் குறியீட்டை உள்ளூர் (எ.கா., Visual Studio Code-இல்) இயக்கலாம், அப்போது சிமுலேஷன் புதிய சாளரத்தில் திறக்கப்படும். ஆன்லைனில் குறியீட்டை இயக்கும்போது, குறியீட்டில் சில மாற்றங்களைச் செய்ய வேண்டி இருக்கலாம், [இங்கே](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7) விவரிக்கப்பட்டது போல.

## OpenAI Gym

முந்தைய பாடத்தில், விளையாட்டின் விதிகள் மற்றும் நிலை `Board` வகுப்பால் கொடுக்கப்பட்டது, இதை நாங்கள் நமக்கே வரையறுத்தோம். இங்கே, சமநிலையை பராமரிக்கும் கம்பத்தின் புவியியல் சிக்கல்களை சிமுலேட் செய்யும் ஒரு சிறப்பு **சிமுலேஷன் சூழல்** பயன்படுத்துவோம். reinforcement learning algorithm-களை பயிற்சி செய்ய மிகவும் பிரபலமான சிமுலேஷன் சூழல்களில் ஒன்று [Gym](https://gym.openai.com/) எனப்படுகிறது, இது [OpenAI](https://openai.com/) மூலம் பராமரிக்கப்படுகிறது. இந்த ஜிம் மூலம், கார்ட்போல் சிமுலேஷனிலிருந்து அடாரி விளையாட்டுகள் வரை பல்வேறு **சூழல்களை** உருவாக்கலாம்.

> **குறிப்பு**: OpenAI Gym-இல் கிடைக்கும் பிற சூழல்களை [இங்கே](https://gym.openai.com/envs/#classic_control) காணலாம்.

முதலில், ஜிம் நிறுவி தேவையான நூலகங்களை இறக்குமதி செய்வோம் (குறியீட்டு தொகுதி 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## பயிற்சி - ஒரு கார்ட்போல் சூழலை ஆரம்பிக்கவும்

கார்ட்போல் சமநிலை பிரச்சினையை கையாள, தொடர்புடைய சூழலை ஆரம்பிக்க வேண்டும். ஒவ்வொரு சூழலும் தொடர்புடையது:

- **Observation space**: சூழலிலிருந்து நாம் பெறும் தகவலின் அமைப்பை வரையறுக்கிறது. கார்ட்போல் பிரச்சினைக்கானது, கம்பத்தின் நிலை, வேகம் மற்றும் பிற மதிப்புகளைப் பெறுகிறோம்.

- **Action space**: சாத்தியமான செயல்களை வரையறுக்கிறது. நமது சூழலில், செயல்பாட்டு இடம் discrete ஆகும், மேலும் இரண்டு செயல்கள் உள்ளன - **இடது** மற்றும் **வலது**. (குறியீட்டு தொகுதி 2)

1. ஆரம்பிக்க, பின்வரும் குறியீட்டை உள்ளிடவும்:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

சூழல் எப்படி செயல்படுகிறது என்பதைப் பார்க்க, 100 படிகள் வரை ஒரு குறுகிய சிமுலேஷனை இயக்குவோம். ஒவ்வொரு படியிலும், எடுக்க வேண்டிய செயல்களில் ஒன்றை வழங்குகிறோம் - இந்த சிமுலேஷனில், `action_space`-இல் இருந்து ஒரு செயலை சீரற்ற முறையில் தேர்ந்தெடுக்கிறோம்.

1. கீழே உள்ள குறியீட்டை இயக்கி அதன் விளைவுகளைப் பாருங்கள்.

    ✅ இந்த குறியீட்டை உள்ளூர் Python நிறுவலில் இயக்குவது விரும்பத்தக்கது! (குறியீட்டு தொகுதி 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    நீங்கள் இதே போன்ற ஒரு படத்தைப் பார்க்க வேண்டும்:

    ![சமநிலை இல்லாத கார்ட்போல்](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. சிமுலேஷன் போது, செயல்பட எப்படி தீர்மானிக்க வேண்டும் என்பதைப் புரிந்து கொள்ள **observations** பெற வேண்டும். உண்மையில், `step` செயல்பாடு தற்போதைய observations, reward function மற்றும் done flag ஆகியவற்றைத் திருப்புகிறது, இது சிமுலேஷனை தொடர்வதற்கான பொருத்தத்தை குறிக்கிறது: (குறியீட்டு தொகுதி 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    நீங்கள் நோட்புக் வெளியீட்டில் இதே போன்றதைப் பார்க்க வேண்டும்:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    சிமுலேஷனின் ஒவ்வொரு படியிலும் திருப்பப்படும் observation vector பின்வரும் மதிப்புகளை கொண்டுள்ளது:
    - கார் நிலை
    - கார் வேகம்
    - கம்பத்தின் கோணம்
    - கம்பத்தின் சுழற்சி வீதம்

1. அந்த எண்களின் குறைந்தபட்ச மற்றும் அதிகபட்ச மதிப்புகளைப் பெறுங்கள்: (குறியீட்டு தொகுதி 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    ஒவ்வொரு சிமுலேஷன் படியிலும் reward மதிப்பு எப்போதும் 1 என்பதை நீங்கள் கவனிக்கலாம். இது நமது நோக்கம் όσο நேரம் சாத்தியமோ, கம்பத்தை ஒரு நியாயமான செங்குத்து நிலைக்கு பராமரிக்க வேண்டும் என்பதற்காக.

    ✅ உண்மையில், CartPole சிமுலேஷன் தீர்க்கப்பட்டது என்று கருதப்படும், நாம் 100 தொடர்ச்சியான முயற்சிகளில் 195 சராசரி reward பெற முடிந்தால்.

## நிலை discretization

Q-Learning இல், ஒவ்வொரு நிலையிலும் என்ன செய்ய வேண்டும் என்பதை வரையறுக்கும் Q-Table உருவாக்க வேண்டும். இதைச் செய்ய, நிலை **discreet** ஆக இருக்க வேண்டும், மேலும் இது முடிவெடுக்கும் discrete மதிப்புகளின் எண்ணிக்கையை கொண்டிருக்க வேண்டும். எனவே, எப்படியாவது **discretize** செய்ய observations-ஐ finite states-க்கு வரைபடம் செய்ய வேண்டும்.

இதற்கான சில வழிகள் உள்ளன:

- **Bins-களில் பிரிக்கவும்**. ஒரு குறிப்பிட்ட மதிப்பின் இடைவெளையை நாம் அறிந்தால், அந்த இடைவெளியை ஒரு எண்ணிக்கையான **bins**-களில் பிரிக்கலாம், பின்னர் அந்த மதிப்பை அது சேர்ந்த bin எண்ணிக்கையால் மாற்றலாம். numpy [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) முறையைப் பயன்படுத்தி இதைச் செய்யலாம். இந்த வழியில், state அளவை நாங்கள் துல்லியமாக அறிந்திருப்போம், ஏனெனில் digitalization-க்கு தேர்ந்தெடுக்கப்பட்ட bins எண்ணிக்கையை இது சார்ந்திருக்கும்.

✅ மதிப்புகளை ஒரு குறிப்பிட்ட இடைவெளிக்கு (எ.கா., -20 முதல் 20 வரை) கொண்டு வர linear interpolation-ஐ பயன்படுத்தலாம், பின்னர் rounding மூலம் எண்களை integers-ஆக மாற்றலாம். இது state அளவின் அளவீட்டில் குறைவான கட்டுப்பாட்டை வழங்குகிறது, குறிப்பாக input மதிப்புகளின் துல்லியமான வரம்புகளை நாம் அறியாதால். உதாரணமாக, நமது சூழலில் 4 மதிப்புகளில் 2-க்கு மேலே/கீழே வரம்புகள் இல்லை, இது states-களின் எண்ணிக்கையை முடிவெடுக்கும் அளவுக்கு அதிகமாக்கலாம்.

நமது உதாரணத்தில், இரண்டாவது அணுகுமுறையைப் பயன்படுத்துவோம். நீங்கள் பின்னர் கவனிக்கலாம், வரையறுக்கப்படாத மேலே/கீழே வரம்புகள் இருந்தாலும், அவை சில குறிப்பிட்ட இடைவெளிகளுக்கு வெளியே மதிப்புகளை அரிதாகவே எடுக்கும், எனவே அந்த states-கள் மிக அரிதாகவே இருக்கும்.

1. நமது மாதிரியில் இருந்து observation-ஐ எடுத்து 4 integer மதிப்புகளின் tuple-ஐ உருவாக்கும் செயல்பாடு இதோ: (குறியீட்டு தொகுதி 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. bins-களைப் பயன்படுத்தி மற்றொரு discretization முறையை ஆராய்வோம்: (குறியீட்டு தொகுதி 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. இப்போது ஒரு குறுகிய சிமுலேஷனை இயக்கி அந்த discrete சூழல் மதிப்புகளைப் பாருங்கள். `discretize` மற்றும் `discretize_bins` இரண்டையும் முயற்சிக்கவும், வேறுபாடு உள்ளதா என்பதைப் பாருங்கள்.

    ✅ `discretize_bins` bin எண்ணிக்கையை திருப்புகிறது, இது 0-ஆதாரமாக உள்ளது. எனவே input variable மதிப்புகள் 0-க்கு அருகில் இருக்கும் போது, இது இடைவெளியின் நடுவில் உள்ள எண்ணிக்கையை (10) திருப்புகிறது. `discretize`-இல், output மதிப்புகளின் வரம்பைப் பற்றி கவலைப்படவில்லை, அவற்றை negative ஆக அனுமதித்தோம், எனவே state மதிப்புகள் shift செய்யப்படவில்லை, மேலும் 0 என்பது 0-க்கு இணையானது. (குறியீட்டு தொகுதி 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    ✅ சூழல் எப்படி செயல்படுகிறது என்பதைப் பார்க்க `env.render` தொடங்கும் வரியை uncomment செய்யவும். இல்லையெனில், அதை பின்னணியில் செயல்படுத்தலாம், இது வேகமாக இருக்கும். Q-Learning செயல்முறையின் போது இந்த "கண்காணிக்க முடியாத" செயல்பாட்டைப் பயன்படுத்துவோம்.

## Q-Table அமைப்பு

முந்தைய பாடத்தில், state 0 முதல் 8 வரை எண்களின் ஒரு எளிய ஜோடி, எனவே Q-Table-ஐ 8x8x2 வடிவத்துடன் numpy tensor மூலம் பிரதிநிதித்துவப்படுத்துவது வசதியாக இருந்தது. bins discretization-ஐ பயன்படுத்தினால், state vector-இன் அளவு அறியப்பட்டதே, எனவே அதே அணுகுமுறையைப் பயன்படுத்தி state-ஐ 20x20x10x10x2 வடிவத்துடன் array மூலம் பிரதிநிதித்துவப்படுத்தலாம் (இங்கே 2 என்பது action space-இன் பரிமாணம், மேலும் முதல் பரிமாணங்கள் observation space-இல் உள்ள ஒவ்வொரு அளவிற்கும் bins-களின் எண்ணிக்கையைப் பிரதிநிதித்துவப்படுத்துகின்றன).

ஆனால், சில நேரங்களில் observation space-இன் துல்லியமான பரிமாணங்கள் அறியப்படவில்லை. `discretize` செயல்பாட்டின் வழக்கில், state குறிப்பிட்ட வரம்புகளுக்குள் இருக்கும் என்பதை நாங்கள் உறுதியாக இருக்க முடியாது, ஏனெனில் சில original values வரம்பற்றவை. எனவே, Q-Table-ஐ dictionary மூலம் பிரதிநிதித்துவப்படுத்த slightly different approach-ஐ பயன்படுத்துவோம்.

1. *(state,action)* ஜோடியை dictionary key ஆகவும், value Q-Table entry value-ஐ பிரதிநிதித்துவப்படுத்தும். (குறியீட்டு தொகுதி 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    இங்கே `qvalues()` எனும் செயல்பாட்டை வரையறுக்கிறோம், இது கொடுக்கப்பட்ட state-க்கு Q-Table மதிப்புகளின் பட்டியலை திருப்புகிறது, இது அனைத்து சாத்தியமான செயல்களுக்கு இணையானது. Q-Table-இல் entry இல்லை என்றால், default-ஆக 0-ஐ திருப்புவோம்.

## Q-Learning தொடங்குவோம்

இப்போது பீட்டர் சமநிலையை கற்றுக்கொள்ள தயாராக இருக்கிறோம்!

1. முதலில், சில hyperparameters-ஐ அமைப்போம்: (குறியீட்டு தொகுதி 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    இங்கே, `alpha` என்பது **learning rate**, இது ஒவ்வொரு படியிலும் Q-Table-இன் தற்போதைய மதிப்புகளை எந்த அளவுக்கு சரிசெய்ய வேண்டும் என்பதை வரையறுக்கிறது. முந்தைய பாடத்தில், 1-இல் தொடங்கி, பயிற்சியின் போது `alpha`-ஐ குறைந்த மதிப்புகளுக்கு குறைத்தோம். இந்த உதாரணத்தில், எளிமைக்காக அதை constant-ஆக வைத்திருப்போம், மேலும் `alpha` மதிப்புகளை சரிசெய்ய முயற்சிக்கலாம்.

    `gamma` என்பது **discount factor**, இது எதிர்கால reward-ஐ தற்போதைய reward-ஐ விட எந்த அளவுக்கு முன்னுரிமை கொடுக்க வேண்டும் என்பதை காட்டுகிறது.

    `epsilon` என்பது **exploration/exploitation factor**, இது exploration-ஐ exploitation-க்கு முன்னுரிமை கொடுக்க வேண்டும் அல்லது மாறாக இருக்க வேண்டும் என்பதை தீர்மானிக்கிறது. நமது algorithm-இல், `epsilon` சதவீதம் வழக்குகளில், Q-Table மதிப்புகளின் அடிப்படையில் அடுத்த செயலை தேர்ந்தெடுப்போம், மேலும் மீதமுள்ள வழக்குகளில் சீரற்ற செயலை செயல்படுத்துவோம். இது நாம் இதுவரை பார்த்திராத தேடல் இடங்களை ஆராய அனுமதிக்கும்.

    ✅ சமநிலையைப் பொருத்தவரை - சீரற்ற செயலை (exploration) தேர்ந்தெடுப்பது தவறான திசையில் சீரற்ற punch ஆக செயல்படும், மேலும் கம்பம் அந்த "தவறுகளிலிருந்து" சமநிலையை மீட்டெடுக்க கற்றுக்கொள்ள வேண்டும்.

### Algorithm-ஐ மேம்படுத்துங்கள்

முந்தைய பாடத்தில் இருந்து algorithm-ஐ இரண்டு மேம்பாடுகளைச் செய்யலாம்:

- **சராசரி மொத்த reward-ஐ கணக்கிடுங்கள்**, ஒரு குறிப்பிட்ட எண்ணிக்கையான சிமுலேஷன்களில். ஒவ்வொரு 5000 iteration-களிலும் முன்னேற்றத்தை அச்சிடுவோம், மேலும் அந்த காலகட்டத்தில் மொத்த reward-ஐ சராசரியாகக் கணக்கிடுவோம். இது 195 புள்ளிகளுக்கு மேல் கிடைத்தால் - பிரச்சினை தீர்க்கப்பட்டது என்று கருதலாம், மேலும் தேவைப்படும் தரத்தை விட அதிகமாக.

- **அதிகபட்ச சராசரி மொத்த முடிவை** `Qmax` கணக்கிடுங்கள், மேலும் அந்த முடிவுக்கு இணையான Q-Table-ஐ சேமிப்போம். பயிற்சியை இயக்கும்போது, சில நேரங்களில் சராசரி மொத்த முடிவு குறையத் தொடங்குகிறது, மேலும் பயிற்சியின் போது காணப்பட்ட சிறந்த மாதிரிக்கு இணையான Q-Table மதிப்புகளை வைத்திருக்க விரும்புகிறோம்.

1. ஒவ்வொரு சிமுலேஷனிலும் மொத்த rewards-ஐ `rewards` vector-இல் சேகரிக்கவும், பின்னர் plotting செய்ய. (குறியீட்டு தொகுதி 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

இந்த முடிவுகளில் இருந்து நீங்கள் கவனிக்கக்கூடியவை:

- **நமது இலக்கிற்கு அருகில்**. 100+ தொடர்ச்சியான சிமுலேஷன் ஓட்டங்களில் 195 மொத்த rewards பெறும் இலக்கை அடைய நாங்கள் மிகவும் அருகில் இருக்கிறோம், அல்லது நாங்கள் உண்மையில் அதை அடைந்திருக்கலாம்! குறைந்த எண்ணிக்கைகளைப் பெறினாலும், நாங்கள் இன்னும் அறியவில்லை, ஏனெனில் 5000 ஓட்டங்களில் சராசரியாக கணக்கிடுகிறோம், மேலும் 100 ஓட்டங்கள் மட்டுமே உத்தியோகபூர்வ அளவீட்டில் தேவைப்படுகிறது.

- **Reward குறையத் தொடங்குகிறது**. சில நேரங்களில் reward குறையத் தொடங்குகிறது, இது Q-Table-இல் ஏற்கனவே கற்றுக்கொண்ட மதிப்புகளை "அழிக்க" முடியும், மேலும் நிலையை மோசமாக்கும் புதிய மதிப்புகளுடன் மாற்றலாம்.

இந்தக் கவனிப்பு பயிற்சியின் முன்னேற்றத்தை plotting செய்யும்போது மேலும் தெளிவாக தெரியும்.

## பயிற்சியின் முன்னேற்றத்தை plotting செய்ய

பயிற்சியின் போது, ஒவ்வொரு iteration-இல் மொத்த reward மதிப்பை `rewards` vector-இல் சேகரித்தோம். iteration எண்ணிக்கைக்கு எதிராக plotting செய்யும்போது இது எப்படி தோன்றுகிறது:

```python
plt.plot(rewards)
```

![raw progress](../../../../translated_images/train_progress_raw.2adfdf2daea09c596fc786fa347a23e9aceffe1b463e2257d20a9505794823ec.ta.png)

இந்த graph-இல் எதையும் சொல்ல முடியாது, ஏனெனில் stochastic training செயல்முறையின் இயல்பால் பயிற்சி அமர்வுகளின் நீளம் மிகவும் மாறுபடுகிறது. இந்த graph-ஐ மேலும் அர்த்தமுள்ளதாக்க, 100 போன்ற ஒரு சோதனைகளின் **running average**-ஐ கணக்கிடலாம். இது `np.convolve`-ஐ பயன்படுத்தி வசதியாக செய்யலாம்: (குறியீட்டு தொகுதி 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![training progress](../../../../translated_images/train_progress_runav.c71694a8fa9ab35935aff6f109e5ecdfdbdf1b0ae265da49479a81b5fae8f0aa.ta.png)

## hyperparameters-ஐ மாறுதல் செய்ய

பயிற்சியை மேலும் நிலையானதாக மாற்ற, சில hyperparameters-ஐ பயிற்சியின் போது சரிசெய்ய makes sense. குறிப்பாக:

- **Learning rate** `alpha`-க்கு, 1-க்கு அருகிலுள்ள மதிப்புகளுடன் தொடங்கலாம், பின்னர் அளவீட்டை குறைத்து வைத்திருக்கலாம். காலத்துடன், Q-Table-இல் நல்ல probability values கிடைக்கும், மேலும் அவற்றை சற்று சரிசெய்ய வேண்டும், புதிய மதிப்புகளுடன்
> **பணி 2**: இந்த பிரச்சினையை முறையாக தீர்க்க, 100 தொடர்ச்சியான இயக்கங்களில் 195 சராசரி வெகுமதியை பெற வேண்டும். பயிற்சியின் போது அதை அளவிடுங்கள் மற்றும் நீங்கள் பிரச்சினையை முறையாக தீர்த்துவிட்டீர்கள் என்பதை உறுதிப்படுத்துங்கள்!

## முடிவுகளை செயல்பாட்டில் காண்பது

பயிற்சி முடிந்த பிறகு, பயிற்சி பெற்ற மாதிரி எப்படி செயல்படுகிறது என்பதை பார்க்குவது சுவாரஸ்யமாக இருக்கும். Q-Table இல் உள்ள சாத்தியமான விநியோகத்தைப் பின்பற்றி, பயிற்சியின் போது பயன்படுத்திய அதே செயல்பாட்டு தேர்வு உத்தியை பின்பற்றி சிமுலேஷனை இயக்குவோம்: (குறியீட்டு பகுதி 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```
  
நீங்கள் இதைப் போன்றதை காணலாம்:

![ஒரு சமநிலையுடன் இருக்கும் கார்ட்போல்](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## 🚀சவால்

> **பணி 3**: இங்கு, நாங்கள் Q-Table இன் இறுதி நகலை பயன்படுத்தினோம், இது சிறந்தது அல்ல. நாங்கள் சிறந்த செயல்திறன் கொண்ட Q-Table ஐ `Qbest` மாறியில் சேமித்துள்ளோம் என்பதை நினைவில் கொள்ளுங்கள்! `Qbest` ஐ `Q` க்கு நகலெடுத்து, சிறந்த செயல்திறன் கொண்ட Q-Table உடன் அதே எடுத்துக்காட்டை முயற்சித்து, வேறுபாட்டைக் கவனிக்கிறீர்களா என்று பாருங்கள்.

> **பணி 4**: இங்கு, ஒவ்வொரு படியிலும் சிறந்த செயல்பாட்டை தேர்வு செய்யாமல், அதற்கான சாத்தியமான விநியோகத்துடன் மாதிரியாக தேர்வு செய்தோம். Q-Table மதிப்பில் அதிகமானதை கொண்ட செயல்பாட்டை எப்போதும் தேர்வு செய்வது பொருத்தமாக இருக்குமா? இது `np.argmax` செயல்பாட்டைப் பயன்படுத்தி, Q-Table மதிப்பில் அதிகமான செயல்பாட்டை கண்டறிய முடியும். இந்த உத்தியை செயல்படுத்தி, சமநிலையை மேம்படுத்துகிறதா என்று பாருங்கள்.

## [பாடத்திற்குப் பிந்தைய வினாடி வினா](https://ff-quizzes.netlify.app/en/ml/)

## பணிக்குறிப்பு  
[மவுண்டன் கார் பயிற்சி செய்யுங்கள்](assignment.md)

## முடிவு

நாம் இப்போது ஒரு விளையாட்டின் விரும்பிய நிலையை வரையறுக்கும் வெகுமதி செயல்பாட்டை வழங்குவதன் மூலம், மற்றும் தேடல் இடத்தை புத்திசாலித்தனமாக ஆராயும் வாய்ப்பை வழங்குவதன் மூலம், முகவர்களை நல்ல முடிவுகளை அடைய பயிற்சி செய்ய கற்றுக்கொண்டுள்ளோம். Q-Learning அல்காரிதத்தை நாங்கள் தனித்துவமான மற்றும் தொடர்ச்சியான சூழல்களில், ஆனால் தனித்துவமான செயல்களுடன் வெற்றிகரமாக பயன்படுத்தியுள்ளோம்.

செயல்பாட்டு நிலையும் தொடர்ச்சியானதாக இருக்கும் சூழ்நிலைகளையும், மற்றும் பார்வை இடம் மிகவும் சிக்கலானதாக இருக்கும் சூழ்நிலைகளையும், உதாரணமாக, அடாரி விளையாட்டு திரைபடம் போன்றவற்றையும் கற்கவும் முக்கியம். இந்த பிரச்சினைகளில், நல்ல முடிவுகளை அடைய, நரம்பியல் நெட்வொர்க்குகள் போன்ற சக்திவாய்ந்த இயந்திர கற்றல் உத்திகளைப் பயன்படுத்த வேண்டும். இந்த மேம்பட்ட தலைப்புகள் நமது வரவிருக்கும் மேம்பட்ட AI பாடநெறியின் பொருள் ஆகும்.

---

**குறிப்பு**:  
இந்த ஆவணம் [Co-op Translator](https://github.com/Azure/co-op-translator) என்ற AI மொழிபெயர்ப்பு சேவையை பயன்படுத்தி மொழிபெயர்க்கப்பட்டுள்ளது. எங்கள் நோக்கம் துல்லியமாக இருக்க வேண்டும் என்பதுதான், ஆனால் தானியங்கி மொழிபெயர்ப்புகளில் பிழைகள் அல்லது துல்லியமின்மைகள் இருக்கக்கூடும் என்பதை தயவுசெய்து கவனத்தில் கொள்ளவும். அதன் தாய்மொழியில் உள்ள மூல ஆவணம் அதிகாரப்பூர்வ ஆதாரமாக கருதப்பட வேண்டும். முக்கியமான தகவல்களுக்கு, தொழில்முறை மனித மொழிபெயர்ப்பு பரிந்துரைக்கப்படுகிறது. இந்த மொழிபெயர்ப்பைப் பயன்படுத்துவதால் ஏற்படும் எந்த தவறான புரிதல்கள் அல்லது தவறான விளக்கங்களுக்கு நாங்கள் பொறுப்பல்ல.