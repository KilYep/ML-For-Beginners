<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "68394b2102d3503882e5e914bd0ff5c1",
  "translation_date": "2025-10-11T11:20:47+00:00",
  "source_file": "8-Reinforcement/1-QLearning/assignment.md",
  "language_code": "ta"
}
-->
# ஒரு நிஜமான உலகம்

எங்கள் சூழலில், பீட்டர் சோர்வடையாமல் அல்லது பசிக்காமல் சுதந்திரமாகச் சுற்றி வர முடிந்தது. ஒரு நிஜமான உலகில், அவன் அவ்வப்போது அமர்ந்து ஓய்வெடுக்கவும், தன்னை உணவூட்டவும் வேண்டும். கீழே உள்ள விதிகளை செயல்படுத்துவதன் மூலம், நம் உலகத்தை நிஜமாக்குவோம்:

1. ஒரு இடத்திலிருந்து மற்றொரு இடத்திற்கு நகரும்போது, பீட்டர் **ஆற்றலை** இழக்கிறார் மற்றும் **சோர்வை** பெறுகிறார்.
2. பீட்டர் ஆப்பிள்களை சாப்பிடுவதன் மூலம் அதிக ஆற்றலைப் பெற முடியும்.
3. பீட்டர் மரத்தின் கீழ் அல்லது புல்வெளியில் (அதாவது மரம் அல்லது புல்வெளி - பச்சை நிறம் கொண்ட இடத்தில் நடந்து செல்வது) ஓய்வெடுப்பதன் மூலம் சோர்வை நீக்க முடியும்.
4. பீட்டர் ஓநாயை கண்டுபிடித்து கொல்ல வேண்டும்.
5. ஓநாயை கொல்ல பீட்டருக்கு குறிப்பிட்ட அளவிலான ஆற்றல் மற்றும் சோர்வு நிலைகள் தேவை, இல்லையெனில் அவன் போரில் தோல்வி அடைவான்.

## வழிமுறைகள்

உங்கள் தீர்வுக்கான தொடக்கப் புள்ளியாக [notebook.ipynb](notebook.ipynb) நோட்புக் பயன்படுத்தவும்.

மேலே உள்ள வெகுமதி செயல்பாட்டை விளையாட்டு விதிகளுக்கு ஏற்ப மாற்றி, reinforcement learning algorithm-ஐ இயக்கி, விளையாட்டை வெல்ல சிறந்த உத்தியை கற்றுக்கொண்டு, உங்கள் algorithm மற்றும் random walk-இன் முடிவுகளை வெற்றி மற்றும் தோல்வி எண்ணிக்கைகளின் அடிப்படையில் ஒப்பிடவும்.

> **குறிப்பு**: உங்கள் புதிய உலகில், நிலை மிகவும் சிக்கலானது, மேலும் மனிதனின் நிலையை தவிர சோர்வு மற்றும் ஆற்றல் நிலைகளையும் உள்ளடக்கியது. நீங்கள் நிலையை (Board, energy, fatigue) என்ற tuple ஆகக் குறிக்கலாம், அல்லது state-க்கு ஒரு class-ஐ வரையறுக்கலாம் (இதை `Board`-இல் இருந்து பெறலாம்), அல்லது [rlboard.py](../../../../8-Reinforcement/1-QLearning/rlboard.py) உள்ள original `Board` class-ஐ மாற்றலாம்.

உங்கள் தீர்வில், random walk strategy-க்கு பொறுப்பான code-ஐ வைத்திருக்கவும், மற்றும் உங்கள் algorithm மற்றும் random walk-இன் முடிவுகளை இறுதியில் ஒப்பிடவும்.

> **குறிப்பு**: இது வேலை செய்ய hyperparameters-ஐ சரிசெய்ய வேண்டியிருக்கும், குறிப்பாக epochs எண்ணிக்கையை. விளையாட்டின் வெற்றி (ஓநாயுடன் போராடுதல்) ஒரு அரிதான நிகழ்வு என்பதால், நீண்ட பயிற்சி நேரத்தை எதிர்பார்க்கலாம்.

## மதிப்பீடு

| அளவுகோல் | சிறந்தது                                                                                                                                                                                             | போதுமானது                                                                                                                                                                                | மேம்பாடு தேவை                                                                                                                          |
| -------- | புதிய உலக விதிகளின் வரையறை, Q-Learning algorithm மற்றும் சில உரை விளக்கங்களுடன் ஒரு நோட்புக் வழங்கப்படுகிறது. Q-Learning random walk-ஐ ஒப்பிடும்போது குறிப்பிடத்தகுந்த முன்னேற்றத்தை அடைகிறது. | நோட்புக் வழங்கப்படுகிறது, Q-Learning செயல்படுத்தப்பட்டுள்ளது மற்றும் random walk-ஐ ஒப்பிடும்போது முன்னேற்றத்தை அடைகிறது, ஆனால் குறிப்பிடத்தகுந்ததாக இல்லை; அல்லது நோட்புக் சரியாக ஆவணப்படுத்தப்படவில்லை மற்றும் code நன்றாக அமைக்கப்படவில்லை | உலக விதிகளை மறுவரையறை செய்ய சில முயற்சிகள் செய்யப்பட்டுள்ளன, ஆனால் Q-Learning algorithm வேலை செய்யவில்லை, அல்லது வெகுமதி செயல்பாடு முழுமையாக வரையறுக்கப்படவில்லை |

---

**அறிவிப்பு**:  
இந்த ஆவணம் [Co-op Translator](https://github.com/Azure/co-op-translator) என்ற AI மொழிபெயர்ப்பு சேவையை பயன்படுத்தி மொழிபெயர்க்கப்பட்டுள்ளது. நாங்கள் துல்லியத்திற்காக முயற்சிக்கிறோம், ஆனால் தானியங்கி மொழிபெயர்ப்புகளில் பிழைகள் அல்லது தவறுகள் இருக்கக்கூடும் என்பதை கவனத்தில் கொள்ளவும். அதன் சொந்த மொழியில் உள்ள மூல ஆவணம் அதிகாரப்பூர்வ ஆதாரமாக கருதப்பட வேண்டும். முக்கியமான தகவல்களுக்கு, தொழில்முறை மனித மொழிபெயர்ப்பு பரிந்துரைக்கப்படுகிறது. இந்த மொழிபெயர்ப்பைப் பயன்படுத்துவதால் ஏற்படும் எந்த தவறான புரிதல்களுக்கும் அல்லது தவறான விளக்கங்களுக்கும் நாங்கள் பொறுப்பல்ல.